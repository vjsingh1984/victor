# Tutorial: Integrating a New LLM Provider - Part 5

**Part 5 of 5:** Error Handling Best Practices and Complete Example

---

## Navigation

- [Part 1: Architecture & Steps 1-2](part-1-provider-architecture.md)
- [Part 2: Streaming & Error Handling](part-2-streaming-error-handling.md)
- [Part 3: Tool Calling Adapter](part-3-tool-calling-adapter.md)
- [Part 4: Registration & Testing](part-4-registration-testing.md)
- **[Part 5: Best Practices & Examples](#)** (Current)
- [**Complete Guide**](integrate-provider.md)

---

## 7. Error Handling Best Practices

### Error Types

Victor provides specific error types for different failure modes:

| Error Type | When to Use |
|------------|-------------|
| `ProviderError` | Base class for all provider errors |
| `ProviderAuthError` | Authentication/authorization failures (401, 403) |
| `ProviderRateLimitError` | Rate limiting (429) |
| `ProviderTimeoutError` | Request timeouts |
| `ProviderConnectionError` | Network connectivity issues |
| `ProviderInvalidResponseError` | Malformed API responses |

### Circuit Breaker

The base class includes circuit breaker support. Use it for API calls:

```python
# Protected API call
response = await self._execute_with_circuit_breaker(
    self.client.post, "/chat/completions", json=payload
)

# Check circuit state
if self.is_circuit_open():
    logger.warning("Circuit breaker is open, failing fast")

# Get circuit stats
stats = self.get_circuit_breaker_stats()
```

### Retry Strategy

For transient failures, implement exponential backoff:

```python
import asyncio
from typing import TypeVar

T = TypeVar("T")

async def retry_with_backoff(
    func,
    max_retries: int = 3,
    base_delay: float = 1.0,
) -> T:
    """Retry a function with exponential backoff."""
    last_exception = None

    for attempt in range(max_retries):
        try:
            return await func()
        except ProviderRateLimitError as e:
            last_exception = e
            if attempt < max_retries - 1:
                delay = base_delay * (2 ** attempt)
                await asyncio.sleep(delay)

    raise last_exception
```

---

## 8. Complete Provider Example

Here is the complete provider implementation file:

```python
# victor/providers/custom_provider.py
"""Complete CustomLLM provider implementation."""

import json
import logging
import os
from typing import Any, AsyncIterator, Dict, List, Optional

import httpx

from victor.providers.base import (
    BaseProvider,
    CompletionResponse,
    Message,
    ProviderError,
    ProviderAuthError,
    ProviderRateLimitError,
    ProviderTimeoutError,
    StreamChunk,
    ToolDefinition,
)

logger = logging.getLogger(__name__)

DEFAULT_BASE_URL = "https://api.customllm.example.com/v1"


class CustomLLMProvider(BaseProvider):
    """Provider for CustomLLM API."""

    DEFAULT_TIMEOUT = 60

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: str = DEFAULT_BASE_URL,
        timeout: int = DEFAULT_TIMEOUT,
        max_retries: int = 3,
        **kwargs: Any,
    ):
        resolved_key = api_key or os.environ.get("CUSTOM_LLM_API_KEY", "")
        if not resolved_key:
            try:
                from victor.config.api_keys import get_api_key
                resolved_key = get_api_key("custom_llm") or ""
            except ImportError:
                pass

        super().__init__(
            api_key=resolved_key,
            base_url=base_url,
            timeout=timeout,
            max_retries=max_retries,
            **kwargs,
        )

        self.client = httpx.AsyncClient(
            base_url=base_url,
            timeout=httpx.Timeout(timeout),
            headers={
                "Authorization": f"Bearer {resolved_key}",
                "Content-Type": "application/json",
            },
        )

    @property
    def name(self) -> str:
        return "custom_llm"

    def supports_tools(self) -> bool:
        return True

    def supports_streaming(self) -> bool:
        return True

    async def chat(
        self,
        messages: List[Message],
        *,
        model: str,
        temperature: float = 0.7,
        max_tokens: int = 4096,
        tools: Optional[List[ToolDefinition]] = None,
        **kwargs: Any,
    ) -> CompletionResponse:
        try:
            payload = self._build_request_payload(
                messages, model, temperature, max_tokens, tools, False, **kwargs
            )

            response = await self._execute_with_circuit_breaker(
                self.client.post, "/chat/completions", json=payload
            )
            response.raise_for_status()

            return self._parse_response(response.json(), model)

        except httpx.TimeoutException as e:
            raise ProviderTimeoutError(
                message=f"Request timed out after {self.timeout}s",
                provider=self.name,
            ) from e
        except httpx.HTTPStatusError as e:
            raise self._handle_http_error(e)
        except Exception as e:
            raise ProviderError(
                message=f"Unexpected error: {str(e)}",
                provider=self.name,
                raw_error=e,
            ) from e

    async def stream(
        self,
        messages: List[Message],
        *,
        model: str,
        temperature: float = 0.7,
        max_tokens: int = 4096,
        tools: Optional[List[ToolDefinition]] = None,
        **kwargs: Any,
    ) -> AsyncIterator[StreamChunk]:
        try:
            payload = self._build_request_payload(
                messages, model, temperature, max_tokens, tools, True, **kwargs
            )

            accumulated_tool_calls: List[Dict[str, Any]] = []

            async with self.client.stream(
                "POST", "/chat/completions", json=payload
            ) as response:
                response.raise_for_status()

                async for line in response.aiter_lines():
                    if not line.strip():
                        continue

                    if line.startswith("data: "):
                        data_str = line[6:]

                        if data_str.strip() == "[DONE]":
                            yield StreamChunk(
                                content="",
                                tool_calls=(
                                    accumulated_tool_calls
                                    if accumulated_tool_calls else None
                                ),
                                stop_reason="stop",
                                is_final=True,
                            )
                            break

                        try:
                            chunk_data = json.loads(data_str)
                            yield self._parse_stream_chunk(
                                chunk_data, accumulated_tool_calls
                            )
                        except json.JSONDecodeError:
                            logger.warning(f"JSON error: {line[:100]}")

        except httpx.TimeoutException as e:
            raise ProviderTimeoutError(
                message=f"Stream timed out",
                provider=self.name,
            ) from e
        except httpx.HTTPStatusError as e:
            raise self._handle_http_error(e)
        except Exception as e:
            raise ProviderError(
                message=f"Stream error: {str(e)}",
                provider=self.name,
                raw_error=e,
            ) from e

    def _build_request_payload(
        self,
        messages: List[Message],
        model: str,
        temperature: float,
        max_tokens: int,
        tools: Optional[List[ToolDefinition]],
        stream: bool,
        **kwargs: Any,
    ) -> Dict[str, Any]:
        formatted_messages = []
        for msg in messages:
            formatted_msg: Dict[str, Any] = {
                "role": msg.role,
                "content": msg.content,
            }
            if msg.tool_call_id:
                formatted_msg["tool_call_id"] = msg.tool_call_id
            formatted_messages.append(formatted_msg)

        payload: Dict[str, Any] = {
            "model": model,
            "messages": formatted_messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": stream,
        }

        if tools:
            payload["tools"] = [
                {
                    "type": "function",
                    "function": {
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": tool.parameters,
                    },
                }
                for tool in tools
            ]
            payload["tool_choice"] = "auto"

        for key, value in kwargs.items():
            if key not in {"api_key"} and value is not None:
                payload[key] = value

        return payload

    def _parse_response(
        self, result: Dict[str, Any], model: str
    ) -> CompletionResponse:
        choices = result.get("choices", [])
        if not choices:
            return CompletionResponse(
                content="", role="assistant", model=model, raw_response=result
            )

        choice = choices[0]
        message = choice.get("message", {})

        usage = None
        usage_data = result.get("usage")
        if usage_data:
            usage = {
                "prompt_tokens": usage_data.get("prompt_tokens", 0),
                "completion_tokens": usage_data.get("completion_tokens", 0),
                "total_tokens": usage_data.get("total_tokens", 0),
            }

        return CompletionResponse(
            content=message.get("content", "") or "",
            role="assistant",
            tool_calls=self._normalize_tool_calls(message.get("tool_calls")),
            stop_reason=choice.get("finish_reason"),
            usage=usage,
            model=model,
            raw_response=result,
        )

    def _normalize_tool_calls(
        self, tool_calls: Optional[List[Dict[str, Any]]]
    ) -> Optional[List[Dict[str, Any]]]:
        if not tool_calls:
            return None

        normalized = []
        for call in tool_calls:
            if "function" in call:
                function = call.get("function", {})
                name = function.get("name")
                arguments = function.get("arguments", "{}")

                if isinstance(arguments, str):
                    try:
                        arguments = json.loads(arguments)
                    except json.JSONDecodeError:
                        arguments = {}

                if name:
                    normalized.append({
                        "id": call.get("id", ""),
                        "name": name,
                        "arguments": arguments,
                    })

        return normalized if normalized else None

    def _parse_stream_chunk(
        self,
        chunk_data: Dict[str, Any],
        accumulated_tool_calls: List[Dict[str, Any]],
    ) -> StreamChunk:
        choices = chunk_data.get("choices", [])
        if not choices:
            return StreamChunk(content="", is_final=False)

        choice = choices[0]
        delta = choice.get("delta", {})
        finish_reason = choice.get("finish_reason")

        for tc_delta in delta.get("tool_calls", []):
            idx = tc_delta.get("index", 0)
            while len(accumulated_tool_calls) <= idx:
                accumulated_tool_calls.append({"id": "", "name": "", "arguments": ""})

            if "id" in tc_delta:
                accumulated_tool_calls[idx]["id"] = tc_delta["id"]
            if "function" in tc_delta:
                func = tc_delta["function"]
                if "name" in func:
                    accumulated_tool_calls[idx]["name"] = func["name"]
                if "arguments" in func:
                    accumulated_tool_calls[idx]["arguments"] += func["arguments"]

        final_tool_calls = None
        if finish_reason and accumulated_tool_calls:
            final_tool_calls = []
            for tc in accumulated_tool_calls:
                if tc.get("name"):
                    args = tc.get("arguments", "{}")
                    try:
                        parsed = json.loads(args) if isinstance(args, str) else args
                    except json.JSONDecodeError:
                        parsed = {}
                    final_tool_calls.append({
                        "id": tc.get("id", ""),
                        "name": tc["name"],
                        "arguments": parsed,
                    })

        return StreamChunk(
            content=delta.get("content", "") or "",
            tool_calls=final_tool_calls,
            stop_reason=finish_reason,
            is_final=finish_reason is not None,
        )

    def _handle_http_error(self, error: httpx.HTTPStatusError) -> ProviderError:
        status = error.response.status_code
        body = error.response.text[:500] if error.response.text else ""

        if status in (401, 403):
            raise ProviderAuthError(
                message=f"Authentication failed: {body}",
                provider=self.name,
                raw_error=error,
            )
        elif status == 429:
            raise ProviderRateLimitError(
                message=f"Rate limit exceeded: {body}",
                provider=self.name,
                status_code=429,
                raw_error=error,
            )
        else:
            raise ProviderError(
                message=f"HTTP error {status}: {body}",
                provider=self.name,
                status_code=status,
                raw_error=error,
            )

    async def close(self) -> None:
        await self.client.aclose()
```

---

## Summary

You have learned how to:

1. Create a provider class inheriting from `BaseProvider`
2. Implement required methods: `name`, `chat()`, `stream()`, `close()`
3. Handle tool calling with proper normalization
4. Configure model capabilities in YAML
5. Register your provider
6. Write comprehensive tests

For more advanced topics, see:
- [Tool Calling Adapters](../guides/development/TOOL_CALLING_FORMATS.md)
- [Circuit Breaker Configuration](../guides/RESILIENCE.md)
- [Provider Registry](../reference/providers/index.md)

---

## Quick Reference

### Provider Checklist

- [ ] Create `victor/providers/{name}_provider.py`
- [ ] Inherit from `BaseProvider`
- [ ] Implement `name` property
- [ ] Implement `chat()` method
- [ ] Implement `stream()` method
- [ ] Implement `close()` method
- [ ] Override `supports_tools()` if applicable
- [ ] Override `supports_streaming()` if applicable
- [ ] Add to `model_capabilities.yaml`
- [ ] Register in `registry.py`
- [ ] Create unit tests
- [ ] Create integration tests (optional)

### Import Locations

```python
# Provider base classes and types
from victor.providers.base import (
    BaseProvider,
    Message,
    CompletionResponse,
    StreamChunk,
    ToolDefinition,
    ProviderError,
    ProviderAuthError,
    ProviderRateLimitError,
    ProviderTimeoutError,
)

# Provider registry
from victor.providers.registry import ProviderRegistry

# Tool calling adapter base
from victor.agent.tool_calling.base import (
    BaseToolCallingAdapter,
    FallbackParsingMixin,
    ToolCallingCapabilities,
    ToolCallFormat,
)
```

---

**Last Updated:** February 01, 2026
**Part 5 of 5**
