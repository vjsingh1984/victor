# Victor v0.4.1 Improvement Plan

## Overview

This document outlines the improvements planned for Victor v0.4.1, based on comprehensive analysis of the SWE-bench evaluation system, tool selection mechanisms, and benchmark infrastructure.

## Completed in v0.4.0

The following features were implemented as part of the v0.4.0 release preparation:

### Phase 2.1: CLI Scaffold Command
- **Files**: `victor/ui/commands/scaffold.py`, `victor/templates/vertical/`
- Jinja2 templates for generating new vertical structures
- `victor new-vertical <name>` CLI command with dry-run support
- Templates: `__init__.py.j2`, `assistant.py.j2`, `safety.py.j2`, `prompts.py.j2`, `mode_config.py.j2`, `service_provider.py.j2`

### Phase 3.1: Entry Points Discovery
- **Files**: `victor/core/verticals/base.py`, `victor/core/verticals/__init__.py`
- `VerticalRegistry.discover_external_verticals()` method
- Entry point group: `victor.verticals`
- Validation of external verticals before registration
- Name conflict handling with built-in verticals

### Phase 4.1: Observability Dashboard
- **Files**: `victor/observability/dashboard/`, `victor/ui/commands/dashboard.py`
- Textual TUI dashboard with tabs: Events, Table, Tools, Verticals, History
- Real-time EventBus subscription
- JSONL log file browser
- `victor dashboard` CLI command with `--demo` mode

### Integration Tests
- **Files**: `tests/integration/test_vertical_plugin_loading.py`, `tests/integration/test_chain_execution.py`
- 32 comprehensive tests for plugin loading system
- Tests for chain execution workflows

---

## Planned for v0.4.1

### Priority 1: Critical Fixes

#### 1.1 Benchmark CLI --parallel Flag - DONE
- **Issue**: The `--parallel` flag in `victor benchmark run` is defined but not implemented
- **Location**: `victor/ui/commands/benchmark.py`
- **Status**: Already implemented in `harness.py:_run_parallel()` using `asyncio.Semaphore` and `asyncio.gather`
- **Effort**: None required

#### 1.2 Progress Persistence for Benchmarks - DONE
- **Issue**: If a benchmark run is interrupted, all progress is lost
- **Location**: `victor/evaluation/harness.py`
- **Fix**: Added checkpoint/resume capability with `--resume` flag
- **Implementation**:
  - `_get_checkpoint_path()`, `_save_checkpoint()`, `_load_checkpoint()`, `_clear_checkpoint()` methods
  - `--resume` flag in CLI to continue from checkpoint
  - Checkpoints stored in `~/.victor/checkpoints/`
- **Effort**: Medium - Complete

#### 1.3 Docker Container Kill Bug
- **Issue**: Potential issue in container cleanup that may kill wrong process
- **Location**: `victor/evaluation/harness.py` (container management)
- **Fix**: Audit container ID handling in cleanup routines
- **Effort**: Low

### Priority 2: High Impact Improvements

#### 2.1 Benchmark-Specific Tool Selection Mode
- **Issue**: Tool selection doesn't distinguish between benchmark and interactive use
- **Location**: `victor/tools/tool_selection.py`, `victor/tools/semantic_selector.py`
- **Fix**: Add `SelectionContext.BENCHMARK` mode with different tool scoring
- **Effort**: Medium

#### 2.2 Task Complexity Detection
- **Issue**: Tool budget isn't adapted based on task complexity
- **Location**: `victor/tools/tool_selection.py`
- **Fix**: Add complexity heuristics to adjust tool budget dynamically
- **Effort**: Medium

#### 2.3 Create Benchmarking Guide - DONE
- **Issue**: No user-facing documentation for running benchmarks
- **Location**: `docs/guides/BENCHMARKING.md`
- **Content**:
  - Setup instructions for SWE-bench
  - Profile configuration for benchmarks
  - Interpreting results
  - Common issues and solutions
- **Status**: Created comprehensive guide covering all benchmarks, CLI options, and troubleshooting
- **Effort**: Low - Complete

### Priority 3: Enhancements

#### 3.1 Patch Validation
- **Issue**: Generated patches aren't validated before application
- **Location**: `victor/evaluation/agent_adapter.py`
- **Fix**: Add `validate_patch()` method to check patch syntax before submission
- **Effort**: Low

#### 3.2 Hook Registration Fallback
- **Issue**: If hook registration fails, no fallback behavior
- **Location**: `victor/evaluation/agent_adapter.py`
- **Fix**: Add graceful degradation when hooks can't be registered
- **Effort**: Low

#### 3.3 Test Output Parser Improvements
- **Issue**: Test output parsing is limited to basic pass/fail
- **Location**: `victor/evaluation/test_runners.py`
- **Fix**: Add structured parsing for pytest, unittest output formats
- **Effort**: Medium

#### 3.4 Coding Workflow Modeling
- **Issue**: Tool selection doesn't model typical coding workflows
- **Location**: `victor/tools/tool_selection.py`
- **Fix**: Add workflow-aware tool ordering (read → analyze → edit → verify)
- **Effort**: Medium

### Priority 4: Documentation & Polish

#### 4.1 Document Vertical Development
- Add entry point registration examples
- Document validation requirements
- Include troubleshooting section

#### 4.2 Add docs/guides/DASHBOARD_USAGE.md
- Dashboard keyboard shortcuts
- Custom event filtering
- JSONL log format specification

#### 4.3 CLI Help Improvements
- Add examples to all `--help` outputs
- Document environment variables

---

## Implementation Order

### Sprint 1 (Week 1-2) - COMPLETED
1. [P1] ~~Fix --parallel flag (1.1)~~ - Already implemented in harness.py
2. [P1] ~~Add progress persistence (1.2)~~ - Added checkpoint/resume capability
3. [P2] ~~Create BENCHMARKING.md (2.3)~~ - Created docs/guides/BENCHMARKING.md

### Sprint 2 (Week 3-4)
1. [P1] Audit Docker kill (1.3)
2. [P2] Benchmark tool selection mode (2.1)
3. [P3] Patch validation (3.1)

### Sprint 3 (Week 5-6)
1. [P2] Task complexity detection (2.2)
2. [P3] Hook registration fallback (3.2)
3. [P3] Test output parser (3.3)

### Sprint 4 (Week 7-8)
1. [P3] Coding workflow modeling (3.4)
2. [P4] Documentation updates (4.1-4.3)
3. Final testing and release preparation

---

## Success Metrics

### Evaluation Accuracy
- Benchmark pass rate should be within 5% of reference implementations
- No false positives in test evaluation

### User Experience
- All CLI commands have `--help` with examples
- Dashboard launches in < 2 seconds
- External vertical discovery logs clear messages

### Reliability
- Interrupted benchmarks can be resumed
- Container cleanup is 100% reliable
- No orphaned processes after benchmark runs

---

## Related Documents

- [CLAUDE.md](../../CLAUDE.md) - Project overview and commands
- [FIRST_RUN.md](../../guides/FIRST_RUN.md) - Getting started guide
- [API_KEYS_CONFIGURATION.md](../../API_KEYS_CONFIGURATION.md) - Provider setup
