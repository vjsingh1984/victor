# =============================================================================
# Victor Configuration - Model Profiles with Detailed Documentation
# =============================================================================
# This is an example configuration file for Victor.
# Copy to ~/.victor/profiles.yaml to activate.
#
# Based on comprehensive model research (Feb 2026):
#   - 40-56B model analysis: /tmp/models_40_56b_complete.md
#   - 30-35B dense models: /tmp/dense_models_30_35b.md
#   - Model comparisons: /tmp/model_comparison_40_56b.md
#   - Optimal sizing for 48GB VRAM: /tmp/optimal_models_m1_max.md
#
# This file contains:
#   1. User profiles (provider, model, temperature, etc.)
#   2. Provider configuration (API keys, endpoints)
#   3. Tool configuration (enabled/disabled tools)
#   4. Model capabilities (tool calling support for each model)
# =============================================================================

profiles:
  # ===========================================================================
  # PRIMARY DEFAULT PROFILE
  # ===========================================================================
  # RECOMMENDATION: Choose based on your hardware and use case
  #
  # For 48GB VRAM (M1 Max/M2 Max):
  #   ✅ qwen2.5-coder-tools:32b-262K (32B dense, 262K context)
  #   ✅ mixtral-tools:8x7b-65K (46.7B MoE, fast)
  #   ✅ deepseek-coder-tools:33b-262K (33B coding specialist)
  #
  # For cloud (API required):
  #   ✅ GLM-5 (40B active, SOTA, MIT license)
  #   ✅ DeepSeek V3 (37B active, massive MoE)
  #   ✅ Qwen3-Coder 480B (massive coding model)
  #
  # KEYS FOR EACH PROFILE:
  #   - provider: ollama, anthropic, openai, google, xai, etc.
  #   - model: model name (e.g., qwen2.5-coder-tools:32b-262K)
  #   - endpoint: URL for Ollama (optional, uses provider default)
  #   - temperature: 0.0-2.0 (lower = more focused, higher = more creative)
  #   - max_tokens: Maximum OUTPUT tokens (not context window!)
  #                   - 4096: Quick tasks
  #                   - 8192: Coding, complex tasks
  #                   - 16384+: Reasoning chains
  #   - description: Human-readable profile description
  # ===========================================================================

  # ---------------------------------------------------------------------------
  # DEFAULT PROFILE - Qwen2.5-Coder 32B (Recommended for 48GB VRAM)
  # ---------------------------------------------------------------------------
  # WHY THIS DEFAULT:
  #   ✅ 32B dense parameters - perfect sweet spot for 48GB VRAM
  #   ✅ 262K context - MASSIVE context window (4x larger than most)
  #   ✅ Near-SOTA performance - competitive with 70B models
  #   ✅ Localhost - no network latency, no API costs
  #   ✅ Tool calling - native function support
  #   ✅ 19GB file - fits with 20GB+ VRAM headroom
  #   ✅ Multilingual - excellent Chinese/English
  #
  # STRENGTHS:
  #   - Best balance of size, context, and performance
  #   - Excellent for 90% of tasks (chat, coding, reasoning, tools)
  #   - Massive 262K context for large codebases/documents
  #   - Near-SOTA benchmarks (HumanEval ~80%, GSM8K ~85%)
  #   - Coding specialist variant available
  #
  # WEAKNESSES:
  #   - Dense model (32B active per token, not sparse like MoE)
  #   - Slower than MoE models for simple tasks
  #   - Not the absolute SOTA (cloud models like GLM-5 are better)
  #
  # USE FOR:
  #   - Daily driver (general use)
  #   - Long-context tasks (large codebases, documents)
  #   - Coding (it's a coder variant)
  #   - Tool workflows
  #   - Chinese/English bilingual
  # ---------------------------------------------------------------------------
  default:
    provider: ollama
    model: qwen2.5-coder-tools:32b-262K
    temperature: 0.7
    max_tokens: 8192
    description: 'DEFAULT: Qwen2.5-Coder 32B (262K context) - Best all-rounder for 48GB VRAM'

  # ===========================================================================
  # DENSE MODELS 30-35B (Predictable Performance)
  # ===========================================================================
  # Dense models activate ALL parameters per token (100% activation).
  # More consistent than MoE, but slower and use more VRAM.
  #
  # RECOMMENDED FOR:
  #   - Predictable performance (all params active)
  #   - Long-context tasks (262K context)
  #   - General-purpose use
  # ===========================================================================

  # ---------------------------------------------------------------------------
  # Qwen2.5 32B - Best All-Rounder Dense
  # ---------------------------------------------------------------------------
  local:
    provider: ollama
    model: qwen2.5-coder-tools:32b-262K
    temperature: 0.7
    max_tokens: 8192
    description: 'Qwen2.5-Coder 32B (262K) - Best all-rounder dense model'

  # ---------------------------------------------------------------------------
  # DeepSeek-Coder 33B - Coding Specialist
  # ---------------------------------------------------------------------------
  coding:
    provider: ollama
    model: deepseek-coder-tools:33b-262K
    temperature: 0.7
    max_tokens: 8192
    description: 'DeepSeek-Coder 33B (262K) - Best open-source coding model'

  # ---------------------------------------------------------------------------
  # Command R 35B - RAG Specialist
  # ---------------------------------------------------------------------------
  rag:
    provider: ollama
    model: command-r:35b
    temperature: 0.7
    max_tokens: 8192
    description: 'Command R 35B (128K) - Optimized for RAG and tools'

  # ===========================================================================
  # Mixture of Experts (MoE) MODELS 40-56B (Efficient Scaling)
  # ===========================================================================
  # MoE models activate only a SUBSET of experts per token (sparse activation).
  # More efficient than dense models, faster for same total parameters.
  #
  # RECOMMENDED FOR:
  #   - Agent-heavy workflows (lots of tool calls)
  #   - When you need larger model capacity
  #   - Faster responses (sparse activation)
  # ===========================================================================

  # ---------------------------------------------------------------------------
  # Mixtral 8x7B - Best MoE for Tool-Based Agents (46.7B)
  # ---------------------------------------------------------------------------
  # WHY CHOOSE:
  #   ✅ 46.7B total parameters (perfect for 40-56B target)
  #   ✅ MoE efficiency - only 11.7B active per token (25%)
  #   ✅ Native tool calling - excellent for agents
  #   ✅ Multilingual - 5 languages supported
  #   ✅ 32-65K context - decent context window
  #   ✅ Apache 2.0 license - fully open
  #
  # STRENGTHS:
  #   - Fast MoE inference (sparse activation)
  #   - Best for tool-heavy agent workflows
  #   - More capacity (46.7B total)
  #   - Native parallel tool calls
  #
  # WEAKNESSES:
  #   - Shorter context (32-65K vs 262K on Qwen2.5)
  #   - Older model (late 2023 vs 2024)
  #
  # USE FOR:
  #   - Agent-heavy workflows (lots of tool calls)
  #   - When you need faster responses
  #   - European languages (French, German, Spanish, Italian)
  # ---------------------------------------------------------------------------
  moe:
    provider: ollama
    model: mixtral-tools:8x7b-65K
    temperature: 0.7
    max_tokens: 8192
    description: 'Mixtral 8x7B (46.7B MoE, 65K) - Best MoE for tool-based agents'

  # ===========================================================================
  # REASONING SPECIALISTS (Chain-of-Thought)
  # ===========================================================================
  # These models excel at complex reasoning, math, and logic.
  # They use "thinking mode" to show their reasoning process.
  #
  # RECOMMENDED FOR:
  #   - Complex problem-solving
  #   - Mathematical reasoning
  #   - Tasks requiring step-by-step logic
  # ===========================================================================

  # ---------------------------------------------------------------------------
  # DeepSeek R1 32B - Reasoning with CoT
  # ---------------------------------------------------------------------------
  reasoning:
    provider: ollama
    model: deepseek-r1-tools:32b-262K
    temperature: 0.7
    max_tokens: 16384
    description: 'DeepSeek-R1 32B (262K) - Chain-of-Thought reasoning'

  # ---------------------------------------------------------------------------
  # Qwen3 30B - Latest Gen with Thinking Mode
  # ---------------------------------------------------------------------------
  qwen3:
    provider: ollama
    model: qwen3-coder-tools:30b-262K
    temperature: 0.7
    max_tokens: 8192
    description: 'Qwen3-Coder 30B (262K) - Latest gen, thinking mode, best Chinese'

  # ===========================================================================
  # CLOUD SOTA MODELS (API Required, Maximum Performance)
  # ===========================================================================
  # These cloud-only models offer state-of-the-art performance.
  # They require API keys and incur costs.
  #
  # RECOMMENDED FOR:
  #   - When you need maximum performance
  #   - Production applications (cost acceptable)
  #   - Complex reasoning/coding at SOTA level
  # ===========================================================================

  # ---------------------------------------------------------------------------
  # GLM-5 - NEW SOTA Model (Feb 2026)
  # ---------------------------------------------------------------------------
  # WHY CHOOSE:
  #   ✅ 744B total, 40B active - EXACT match for 40-56B target!
  #   ✅ SOTA benchmarks - 92.7% AIME, 77.8% SWE-bench
  #   ✅ 198K context - massive context window
  #   ✅ MIT license - commercial-friendly
  #
  # STRENGTHS:
  #   - State-of-the-art performance
  #   - Perfect 40B active parameters (your target!)
  #   - MIT license (fully open)
  #
  # WEAKNESSES:
  #   - Cloud-only (not downloadable)
  #   - API costs
  #
  # USE FOR:
  #   - Maximum performance needed
  #   - Complex reasoning (92.7% AIME!)
  #   - SOTA coding (77.8% SWE-bench!)
  # ---------------------------------------------------------------------------
  glm5:
    provider: ollama
    model: glm-5:cloud
    temperature: 0.7
    max_tokens: 8192
    description: 'GLM-5 (744B total, 40B active) - NEW SOTA, 198K context, MIT'

  # ---------------------------------------------------------------------------
  # DeepSeek V3 - Massive MoE (Cloud)
  # ---------------------------------------------------------------------------
  deepseek-v3:
    provider: fireworks
    model: accounts/fireworks/models/deepseek-v3p2
    temperature: 0.7
    max_tokens: 8192
    description: 'DeepSeek V3.2 (685B MoE, 37B active) - SOTA reasoning, 256K'

  # ---------------------------------------------------------------------------
  # Qwen3-Coder 480B - Massive MoE Coding
  # ---------------------------------------------------------------------------
  qwen3-massive:
    provider: fireworks
    model: accounts/fireworks/models/qwen3-coder-480b-a35b-instruct
    temperature: 0.7
    max_tokens: 8192
    description: 'Qwen3-Coder 480B MoE - Massive coding model'

  # ===========================================================================
  # CLASSIC CLOUD MODELS (Claude, GPT, Gemini, etc.)
  # ===========================================================================

  claude:
    provider: anthropic
    model: claude-sonnet-4-5
    temperature: 1.0
    max_tokens: 8192
    description: 'Claude Sonnet 4.5 - Best for complex tasks'

  gpt-4.1:
    provider: openai
    model: gpt-4.1
    temperature: 0.7
    max_tokens: 8192
    description: 'GPT-4.1 - Latest OpenAI flagship'

  o3:
    provider: openai
    model: o3
    temperature: 1.0
    max_tokens: 65536
    description: 'OpenAI o3 - Latest reasoning model'

  gemini-2.5-pro:
    provider: google
    model: gemini-2.5-pro
    temperature: 1.0
    max_tokens: 8192
    description: 'Gemini 2.5 Pro - 1M context window'

  grok:
    provider: xai
    model: grok-3
    temperature: 0.7
    max_tokens: 16384
    description: 'Grok 3 - 128K context, tools'

  kimi:
    provider: moonshot
    model: kimi-k2-thinking
    temperature: 1.0
    max_tokens: 8192
    description: 'Kimi K2 Thinking - 256K context'

  # ===========================================================================
  # FAST PROFILES (Ultra-Fast Inference)
  # ===========================================================================

  quick:
    provider: ollama
    model: mistral-tools:7b-instruct
    temperature: 0.5
    max_tokens: 2048
    description: 'Mistral 7B - Fastest local model'

  groq:
    provider: groqcloud
    model: llama-3.3-70b-versatile
    temperature: 0.7
    max_tokens: 4096
    description: 'Groq: Llama 3.3 70B - Ultra-fast cloud'

  groq-fast:
    provider: groqcloud
    model: llama-3.1-8b-instant
    temperature: 0.7
    max_tokens: 2048
    description: 'Groq: Llama 3.1 8B - Fastest cloud'

  cerebras:
    provider: cerebras
    model: gpt-oss-120b
    temperature: 0.7
    max_tokens: 4096
    description: 'Cerebras: GPT-OSS 120B - Ultra-fast (free tier)'

# =============================================================================
# PROVIDERS - API Keys and Endpoints
# =============================================================================
providers:
  ollama:
    base_url:
      - http://192.168.1.227:11434  # Primary (LAN server)
      - http://192.168.1.20:11434    # Secondary (LAN server)
      - http://localhost:11434        # Fallback (localhost)
    timeout: 300

  anthropic:
    api_key: ${ANTHROPIC_API_KEY}

  openai:
    api_key: ${OPENAI_API_KEY}

  google:
    api_key: ${GOOGLE_API_KEY}

  xai:
    api_key: ${XAI_API_KEY}

  moonshot:
    api_key: ${MOONSHOT_API_KEY}
    base_url: https://api.moonshot.cn/v1
    timeout: 120

  deepseek:
    api_key: ${DEEPSEEK_API_KEY}
    base_url: https://api.deepseek.com/v1
    timeout: 120

  groqcloud:
    api_key: ${GROQCLOUD_API_KEY}
    base_url: https://api.groq.com/openai/v1
    timeout: 60

  cerebras:
    api_key: ${CEREBRAS_API_KEY}
    base_url: https://api.cerebras.ai/v1
    timeout: 60

  fireworks:
    api_key: ${FIREWORKS_API_KEY}
    base_url: https://api.fireworks.ai/inference/v1
    timeout: 120

# =============================================================================
# TOOLS - Enable/Disable Specific Tools
# =============================================================================
tools:
  disabled: []

# =============================================================================
# WEB TOOLS CONFIGURATION
# =============================================================================
web_tools:
  summarize_fetch_top: 5
  summarize_fetch_pool: 10
  summarize_max_content_length: 50000

# =============================================================================
# MODEL CAPABILITIES - Tool Calling Support Per Model
# =============================================================================
# See documentation in ~/.victor/profiles.yaml for details
# =============================================================================
model_capabilities:
  defaults:
    native_tool_calls: false
    streaming_tool_calls: false
    parallel_tool_calls: false
    thinking_mode: false
    requires_strict_prompting: true
    recommended_tool_budget: 10

  providers:
    anthropic:
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    openai:
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    google:
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    xai:
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    moonshot:
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 30

    deepseek:
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    groqcloud:
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    cerebras:
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    fireworks:
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    ollama:
      native_tool_calls: false
      requires_strict_prompting: true
      recommended_tool_budget: 12

  models:
    "*-tools:*":
      native_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    "*-tools":
      native_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    "qwen3*":
      native_tool_calls: true
      parallel_tool_calls: true
      thinking_mode: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    "qwen2.5*":
      native_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    "llama3.1*":
      native_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    "llama3.2*":
      native_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    "llama3.3*":
      native_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    "llama-4.*":
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 25

    "glm-4.*":
      native_tool_calls: true
      parallel_tool_calls: true
      streaming_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    "glm4*":
      native_tool_calls: true
      parallel_tool_calls: true
      streaming_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    "gemma3*":
      native_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 12

    "phi-4.*":
      native_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    "phi4*":
      native_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    "deepseek*":
      native_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    "deepseek-v3*":
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    "deepseek-r1*":
      native_tool_calls: true
      parallel_tool_calls: true
      thinking_mode: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    "deepseek-reasoner*":
      native_tool_calls: false
      thinking_mode: true
      requires_strict_prompting: false
      recommended_tool_budget: 5

    "mistral*":
      native_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 15

    "mixtral*":
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    "devstral*":
      native_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    "kimi-k2*":
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      thinking_mode: true
      requires_strict_prompting: false
      recommended_tool_budget: 30

    "grok*":
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 20

    "llama-3.3-70b-versatile*":
      native_tool_calls: true
      streaming_tool_calls: true
      parallel_tool_calls: true
      requires_strict_prompting: false
      recommended_tool_budget: 25

    "codellama*":
      native_tool_calls: false
      requires_strict_prompting: true
      recommended_tool_budget: 8

    "phi*":
      native_tool_calls: false
      requires_strict_prompting: true
      recommended_tool_budget: 8

# =============================================================================
# COORDINATOR ORCHESTRATOR
# =============================================================================
use_coordinator_orchestrator: true
