# Victor Configuration for Docker
# This file is automatically generated in the Docker container

profiles:
  # Default profile - Uses Ollama
  default:
    provider: ollama
    model: qwen2.5-coder:7b
    temperature: 0.7
    max_tokens: 4096

  # Ollama profiles
  ollama:
    provider: ollama
    model: qwen2.5-coder:7b
    temperature: 0.7
    max_tokens: 4096

  ollama-fast:
    provider: ollama
    model: qwen2.5-coder:1.5b
    temperature: 0.5
    max_tokens: 2048

  # vLLM profile
  vllm:
    provider: openai
    model: Qwen/Qwen2.5-Coder-1.5B-Instruct
    temperature: 0.7
    max_tokens: 2048

  # Cloud providers (requires API keys)
  claude:
    provider: anthropic
    model: claude-sonnet-4-5
    temperature: 1.0
    max_tokens: 8192

  gpt4:
    provider: openai
    model: gpt-4-turbo-preview
    temperature: 0.7
    max_tokens: 4096

  gemini:
    provider: google
    model: gemini-pro
    temperature: 0.9
    max_tokens: 4096

providers:
  # Ollama - Local server
  ollama:
    base_url: ${OLLAMA_HOST:-http://localhost:11434}

  # vLLM - High-performance inference
  openai:
    base_url: ${VLLM_API_BASE:-http://localhost:8000/v1}
    api_key: EMPTY

  # Cloud providers
  anthropic:
    api_key: ${ANTHROPIC_API_KEY}

  google:
    api_key: ${GOOGLE_API_KEY}
