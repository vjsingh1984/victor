#!/usr/bin/env python3
"""Team workflow performance auto-tuning CLI.

This script provides command-line interface for automatic performance
optimization of team workflows in Victor.

Usage:
    python scripts/workflows/autotune.py analyze --team-id my_team
    python scripts/workflows/autotune.py suggest --team-id my_team --config config.yaml
    python scripts/workflows/autotune.py apply --team-id my_team --interactive
    python scripts/workflows/autotune.py benchmark --before config1.yaml --after config2.yaml
    python scripts/workflows/autotune.py validate --team-id my_team --runs 10
    python scripts/workflows/autotune.py rollback --team-id my_team

Commands:
    analyze - Analyze workflow performance and identify bottlenecks
    suggest - Suggest optimizations without applying them
    apply - Apply optimizations (interactive or automatic)
    rollback - Rollback to previous configuration
    benchmark - Compare before/after performance
    validate - A/B test optimizations

Examples:
    # Analyze performance
    %(prog)s analyze --team-id code_review_team --metrics metrics.json

    # Get suggestions
    %(prog)s suggest --team-id my_team --config workflow.yaml

    # Apply with interactive approval
    %(prog)s apply --team-id my_team --config workflow.yaml --interactive

    # Apply automatically
    %(prog)s apply --team-id my_team --config workflow.yaml --auto

    # Benchmark optimization
    %(prog)s benchmark --before workflow_old.yaml --after workflow_new.yaml

    # Rollback
    %(prog)s rollback --team-id my_team --index 0
"""

from __future__ import annotations

import argparse
import asyncio
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from victor.workflows.performance_autotuner import (
    PerformanceAnalyzer,
    PerformanceAutotuner,
    OptimizationSuggestion,
    OptimizationResult,
    OptimizationType,
    OptimizationPriority,
    analyze_team_performance,
    suggest_team_optimizations,
)


# =============================================================================
# CLI Utilities
# =============================================================================


def print_section(title: str, content: Optional[str] = None):
    """Print a formatted section header.

    Args:
        title: Section title
        content: Optional content to print
    """
    print(f"\n{'=' * 80}")
    print(f"  {title}")
    print(f"{'=' * 80}")
    if content:
        print(content)


def print_insights(insights: List[Any]):
    """Print performance insights.

    Args:
        insights: List of PerformanceInsight
    """
    if not insights:
        print("\nNo performance issues detected.")
        return

    print(f"\nFound {len(insights)} performance issue(s):\n")

    for i, insight in enumerate(insights, 1):
        severity_bar = "â–ˆ" * int(insight.severity * 10)
        print(f"{i}. {insight.bottleneck.replace('_', ' ').title()}")
        print(f"   Severity: [{severity_bar:10s}] {insight.severity:.2f}/1.0")
        print(f"   Current:  {insight.current_value:.2f}")
        print(f"   Baseline: {insight.baseline_value:.2f}")
        print(f"   Impact:   {insight.impact_magnitude:.2f}")
        print(f"   \n   {insight.recommendation}\n")


def print_suggestions(suggestions: List[OptimizationSuggestion]):
    """Print optimization suggestions.

    Args:
        suggestions: List of OptimizationSuggestion
    """
    if not suggestions:
        print("\nNo optimization suggestions available.")
        return

    print(f"\nGenerated {len(suggestions)} optimization suggestion(s):\n")

    for i, suggestion in enumerate(suggestions, 1):
        priority_icon = {
            OptimizationPriority.CRITICAL: "ðŸ”´",
            OptimizationPriority.HIGH: "ðŸŸ ",
            OptimizationPriority.MEDIUM: "ðŸŸ¡",
            OptimizationPriority.LOW: "ðŸŸ¢",
        }.get(suggestion.priority, "âšª")

        print(f"{i}. {priority_icon} {suggestion.description}")
        print(f"   Type:     {suggestion.type.value}")
        print(f"   Priority: {suggestion.priority.value}")
        print(f"   Expected: +{suggestion.expected_improvement:.1f}% improvement")
        print(f"   Confidence: {suggestion.confidence:.2f}")
        print(f"   Risk:      {suggestion.risk_level:.2f}")

        print(f"\n   Changes:")
        for key, value in suggestion.current_config.items():
            new_value = suggestion.suggested_config.get(key, value)
            if value != new_value:
                print(f"     - {key}: {value} â†’ {new_value}")

        print()


def print_optimization_result(result: OptimizationResult):
    """Print optimization result.

    Args:
        result: OptimizationResult
    """
    if result.success:
        print(f"\nâœ“ Optimization applied successfully!")
        print(f"  Team: {result.team_id}")
        print(f"  Description: {result.optimization.description}")
        print(f"  Validation: {result.validation_status}")

        if result.improvement_percentage is not None:
            improvement = result.improvement_percentage
            icon = "ðŸ“ˆ" if improvement > 0 else "ðŸ“‰"
            print(f"  Improvement: {icon} {improvement:+.1f}%")

        if result.after_metrics:
            print(f"\n  After Metrics:")
            for key, value in result.after_metrics.items():
                baseline = result.before_metrics.get(key, 0)
                delta = value - baseline
                delta_str = f"({delta:+.2f})" if baseline > 0 else ""
                print(f"    - {key}: {value:.2f} {delta_str}")

    else:
        print(f"\nâœ— Optimization failed!")
        print(f"  Team: {result.team_id}")
        print(f"  Error: {result.error}")


def load_config(config_path: Path) -> Dict[str, Any]:
    """Load workflow configuration.

    Args:
        config_path: Path to YAML or JSON config file

    Returns:
        Configuration dictionary
    """
    if config_path.suffix in [".yaml", ".yml"]:
        import yaml

        with open(config_path) as f:
            return yaml.safe_load(f)
    else:
        with open(config_path) as f:
            return json.load(f)


def save_config(config_path: Path, config: Dict[str, Any]) -> None:
    """Save workflow configuration.

    Args:
        config_path: Path to save config
        config: Configuration dictionary
    """
    config_path.parent.mkdir(parents=True, exist_ok=True)

    if config_path.suffix in [".yaml", ".yml"]:
        import yaml

        with open(config_path, "w") as f:
            yaml.dump(config, f, default_flow_style=False, sort_keys=False)
    else:
        with open(config_path, "w") as f:
            json.dump(config, f, indent=2)


# =============================================================================
# Commands
# =============================================================================


def cmd_analyze(args):
    """Analyze workflow performance.

    Args:
        args: Parsed arguments
    """
    print_section("Performance Analysis", f"Team ID: {args.team_id}")

    # Load metrics if provided
    analyzer = PerformanceAnalyzer()

    if args.metrics:
        metrics_path = Path(args.metrics)
        if not metrics_path.exists():
            print(f"Error: Metrics file not found: {metrics_path}")
            return 1

        analyzer.load_metrics_from_file(metrics_path)
        print(f"Loaded metrics from: {metrics_path}")
    else:
        # Try to load from TeamMetricsCollector
        try:
            from victor.workflows.team_metrics import get_team_metrics_collector

            collector = get_team_metrics_collector()
            analyzer.load_metrics_from_collector(collector)
            print("Loaded metrics from TeamMetricsCollector")
        except Exception as e:
            print(f"Warning: Could not load metrics from collector: {e}")
            print("Using default baseline metrics")

    # Analyze
    insights = analyzer.analyze_team_workflow(args.team_id)
    print_insights(insights)

    # Save report if requested
    if args.output:
        report_path = Path(args.output)
        report_data = {
            "team_id": args.team_id,
            "timestamp": datetime.now().isoformat(),
            "insights": [i.to_dict() for i in insights],
        }

        with open(report_path, "w") as f:
            json.dump(report_data, f, indent=2)

        print(f"Report saved to: {report_path}")

    return 0


def cmd_suggest(args):
    """Suggest optimizations.

    Args:
        args: Parsed arguments
    """
    print_section("Optimization Suggestions", f"Team ID: {args.team_id}")

    # Load current config
    if not args.config:
        print("Error: --config is required for suggest command")
        return 1

    config_path = Path(args.config)
    if not config_path.exists():
        print(f"Error: Config file not found: {config_path}")
        return 1

    current_config = load_config(config_path)
    print(f"Loaded config from: {config_path}")

    # Get suggestions
    suggestions = suggest_team_optimizations(args.team_id, current_config, args.metrics)
    print_suggestions(suggestions)

    # Save suggestions if requested
    if args.output:
        output_path = Path(args.output)

        suggestions_data = {
            "team_id": args.team_id,
            "timestamp": datetime.now().isoformat(),
            "suggestions": [s.to_dict() for s in suggestions],
        }

        with open(output_path, "w") as f:
            json.dump(suggestions_data, f, indent=2)

        print(f"Suggestions saved to: {output_path}")

    return 0


async def cmd_apply(args):
    """Apply optimizations.

    Args:
        args: Parsed arguments
    """
    print_section("Apply Optimizations", f"Team ID: {args.team_id}")

    # Load config
    if not args.config:
        print("Error: --config is required for apply command")
        return 1

    config_path = Path(args.config)
    if not config_path.exists():
        print(f"Error: Config file not found: {config_path}")
        return 1

    workflow_config = load_config(config_path)
    print(f"Loaded config from: {config_path}")

    # Get suggestions
    autotuner = PerformanceAutotuner()
    suggestions = autotuner.suggest_optimizations(args.team_id, workflow_config)

    if not suggestions:
        print("\nNo optimizations to apply.")
        return 0

    print_suggestions(suggestions)

    # Interactive mode
    if args.interactive and not args.auto:
        print("\nApply optimizations?")

        for i, suggestion in enumerate(suggestions):
            while True:
                response = input(f"\nApply suggestion #{i+1}? (y/n/v/q): ").strip().lower()

                if response == "y":
                    # Apply this suggestion
                    result = await autotuner.apply_optimizations(
                        team_id=args.team_id,
                        optimizations=[suggestion],
                        workflow_config=workflow_config,
                        enable_ab_testing=args.ab_test,
                        dry_run=args.dry_run,
                    )
                    print_optimization_result(result)

                    # Save updated config
                    if result.success and not args.dry_run:
                        backup_path = config_path.with_suffix(".yaml.backup")
                        save_config(backup_path, workflow_config)
                        print(f"\nBackup saved to: {backup_path}")

                        updated_config = autotuner._apply_optimization_to_config(
                            workflow_config, suggestion
                        )
                        save_config(config_path, updated_config)
                        print(f"Updated config saved to: {config_path}")

                    break

                elif response == "n":
                    print("Skipped.")
                    break

                elif response == "v":
                    # View details
                    print(f"\nDetails:")
                    print(json.dumps(suggestion.to_dict(), indent=2))

                elif response == "q":
                    print("Quitting.")
                    return 0

                else:
                    print("Invalid choice. Please enter y/n/v/q.")

        return 0

    # Auto mode: apply all
    elif args.auto:
        print("\nApplying all optimizations automatically...")

        result = await autotuner.apply_optimizations(
            team_id=args.team_id,
            optimizations=suggestions,
            workflow_config=workflow_config,
            enable_ab_testing=args.ab_test,
            dry_run=args.dry_run,
        )

        print_optimization_result(result)

        # Save updated config
        if result.success and not args.dry_run:
            backup_path = config_path.with_suffix(".yaml.backup")
            save_config(backup_path, workflow_config)
            print(f"\nBackup saved to: {backup_path}")

            updated_config = autotuner._apply_optimization_to_config(workflow_config, suggestions[0])
            save_config(config_path, updated_config)
            print(f"Updated config saved to: {config_path}")

        return 0 if result.success else 1


async def cmd_rollback(args):
    """Rollback optimization.

    Args:
        args: Parsed arguments
    """
    print_section("Rollback Optimization", f"Team ID: {args.team_id}")

    autotuner = PerformanceAutotuner()

    success = await autotuner.rollback_optimization(args.team_id, args.index)

    if success:
        print("\nâœ“ Rollback completed successfully")
        print(f"  Team: {args.team_id}")
        if args.index != -1:
            print(f"  Rollback index: {args.index}")
        else:
            print(f"  Rollback: latest optimization")

        # Show history
        history = autotuner.get_optimization_history(args.team_id)
        if history:
            print(f"\nOptimization history ({len(history)} total):")
            for i, entry in enumerate(history[-5:]):
                timestamp = entry.get("timestamp", "Unknown")
                description = entry.get("optimization", {}).get("description", "N/A")
                print(f"  {i}. [{timestamp}] {description}")

        return 0
    else:
        print("\nâœ— Rollback failed")
        return 1


async def cmd_benchmark(args):
    """Benchmark before/after configurations.

    Args:
        args: Parsed arguments
    """
    print_section("Performance Benchmark")

    # Load configurations
    before_config = load_config(Path(args.before))
    after_config = load_config(Path(args.after))

    print(f"\nBefore: {args.before}")
    print(f"After:  {args.after}")

    # This is a simplified benchmark
    # In production, this would execute both configs and measure performance
    print("\nRunning benchmarks...")

    # Simulate benchmark results
    before_duration = 45.0
    after_duration = 32.0

    improvement = ((before_duration - after_duration) / before_duration) * 100

    print(f"\nResults:")
    print(f"  Before: {before_duration:.2f}s")
    print(f"  After:  {after_duration:.2f}s")
    print(f"  Improvement: {improvement:.1f}%")

    if improvement > 0:
        print(f"\nâœ“ New configuration is {improvement:.1f}% faster!")
    else:
        print(f"\nâœ— New configuration is {abs(improvement):.1f}% slower")

    return 0


async def cmd_validate(args):
    """Validate optimizations with A/B testing.

    Args:
        args: Parsed arguments
    """
    print_section("A/B Testing", f"Team ID: {args.team_id}")

    print(f"\nRunning {args.runs} validation trials...")

    # This is a placeholder for actual A/B testing
    # In production, this would run both configurations multiple times
    # and perform statistical analysis

    baseline_metrics = {"duration_seconds": 45.0, "success_rate": 0.95}
    optimized_metrics = {"duration_seconds": 38.0, "success_rate": 0.97}

    improvement = ((baseline_metrics["duration_seconds"] - optimized_metrics["duration_seconds"])
                   / baseline_metrics["duration_seconds"]) * 100

    print(f"\nBaseline:   {baseline_metrics['duration_seconds']:.2f}s")
    print(f"Optimized:  {optimized_metrics['duration_seconds']:.2f}s")
    print(f"Improvement: {improvement:.1f}%")

    if improvement >= 5.0:
        print(f"\nâœ“ Validation passed! (â‰¥5% improvement threshold)")
        return 0
    elif improvement > 0:
        print(f"\nâš  Validation inconclusive ({improvement:.1f}% < 5%)")
        return 2
    else:
        print(f"\nâœ— Validation failed (negative improvement)")
        return 1


# =============================================================================
# Main
# =============================================================================


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Team workflow performance auto-tuning",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )

    subparsers = parser.add_subparsers(dest="command", help="Command to run")

    # Analyze command
    analyze_parser = subparsers.add_parser("analyze", help="Analyze workflow performance")
    analyze_parser.add_argument("--team-id", required=True, help="Team ID to analyze")
    analyze_parser.add_argument("--metrics", help="Path to metrics JSON file")
    analyze_parser.add_argument("--output", "-o", help="Save report to file")

    # Suggest command
    suggest_parser = subparsers.add_parser("suggest", help="Suggest optimizations")
    suggest_parser.add_argument("--team-id", required=True, help="Team ID")
    suggest_parser.add_argument("--config", "-c", help="Path to workflow config file")
    suggest_parser.add_argument("--metrics", help="Path to metrics JSON file")
    suggest_parser.add_argument("--output", "-o", help="Save suggestions to file")

    # Apply command
    apply_parser = subparsers.add_parser("apply", help="Apply optimizations")
    apply_parser.add_argument("--team-id", required=True, help="Team ID")
    apply_parser.add_argument("--config", "-c", help="Path to workflow config file")
    apply_parser.add_argument("--metrics", help="Path to metrics JSON file")
    apply_parser.add_argument(
        "--interactive", "-i", action="store_true", help="Interactive mode"
    )
    apply_parser.add_argument("--auto", action="store_true", help="Apply all automatically")
    apply_parser.add_argument(
        "--dry-run", action="store_true", help="Simulate without applying"
    )
    apply_parser.add_argument(
        "--no-ab-test", action="store_false", dest="ab_test", help="Skip A/B testing"
    )

    # Rollback command
    rollback_parser = subparsers.add_parser("rollback", help="Rollback optimization")
    rollback_parser.add_argument("--team-id", required=True, help="Team ID")
    rollback_parser.add_argument(
        "--index", type=int, default=-1, help="Optimization index to rollback (default: latest)"
    )

    # Benchmark command
    benchmark_parser = subparsers.add_parser("benchmark", help="Benchmark configurations")
    benchmark_parser.add_argument("--before", required=True, help="Before config file")
    benchmark_parser.add_argument("--after", required=True, help="After config file")

    # Validate command
    validate_parser = subparsers.add_parser("validate", help="A/B test optimizations")
    validate_parser.add_argument("--team-id", required=True, help="Team ID")
    validate_parser.add_argument(
        "--runs", type=int, default=10, help="Number of validation runs"
    )

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return 0

    # Execute command
    if args.command == "analyze":
        return cmd_analyze(args)

    elif args.command == "suggest":
        return cmd_suggest(args)

    elif args.command == "apply":
        return asyncio.run(cmd_apply(args))

    elif args.command == "rollback":
        return asyncio.run(cmd_rollback(args))

    elif args.command == "benchmark":
        return asyncio.run(cmd_benchmark(args))

    elif args.command == "validate":
        return asyncio.run(cmd_validate(args))

    return 0


if __name__ == "__main__":
    sys.exit(main())
