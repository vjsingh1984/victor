# AlertManager Configuration for Victor Production

global:
  resolve_timeout: 5m
  smtp_from: 'victor-alerts@example.com'
  smtp_smarthost: 'smtp.example.com:587'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: 'CHANGEME'

# Templates for notifications
templates:
  - '/etc/alertmanager/*.tmpl'

# Route tree for alerts
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'

  routes:
    # Critical alerts go to pagerduty/slack
    - match:
        severity: critical
      receiver: 'critical-alerts'
      continue: true

    # Warning alerts go to slack
    - match:
        severity: warning
      receiver: 'warning-alerts'
      continue: true

# Receivers define notification destinations
receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://localhost:5001/alerts'
        send_resolved: true

  - name: 'critical-alerts'
    # PagerDuty integration (configure with your key)
    # pagerduty_configs:
    #   - service_key: 'YOUR_PAGERDUTY_KEY'
    #     description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'

    # Slack integration (configure with your webhook)
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#victor-critical-alerts'
        title: 'CRITICAL: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true

  - name: 'warning-alerts'
    # Slack integration for warnings
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#victor-alerts'
        title: 'WARNING: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true

    # Email notifications (configure SMTP above)
    email_configs:
      - to: 'team@example.com'
        from: 'victor-alerts@example.com'
        headers:
          Subject: 'Victor Alert: {{ .GroupLabels.alertname }}'

# Inhibition rules to prevent alert fatigue
inhibit_rules:
  # Inhibit warning if critical alert is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']
