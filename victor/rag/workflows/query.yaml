# RAG Query Workflow
# ==================
# Retrieval-augmented generation for answering questions from indexed documents.
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ EXECUTION ENVIRONMENT                                                        │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Default: in-process (Python with vector store client)                        │
# │ Supported: in-process, subprocess, docker                                    │
# │                                                                              │
# │ Requirements:                                                                │
# │   - Vector store access (LanceDB/ChromaDB local, or Pinecone/Qdrant cloud)  │
# │   - LLM API access for answer generation (Anthropic/OpenAI)                 │
# │   - Embedding API for query embedding (same as ingestion model)             │
# │                                                                              │
# │ Network requirements:                                                        │
# │   - REQUIRED: LLM API calls for agent nodes                                 │
# │   - REQUIRED: Embedding API for query vectorization                         │
# │   - OPTIONAL: Cloud vector store queries (if not using local)               │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ DEFAULT VALUES                                                               │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ top_k: 10 (chunks to retrieve)                                              │
# │ rerank_top_k: 5 (chunks after reranking)                                    │
# │ similarity_threshold: 0.7 (minimum relevance score)                         │
# │ max_context_tokens: 4000 (context window for answer generation)             │
# │ answer_max_tokens: 2000 (maximum answer length)                             │
# │ temperature: 0.3 (for answer generation - factual)                          │
# │ citation_style: inline (e.g., "according to [1]...")                        │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ COMPUTE vs AGENT NODE RATIONALE                                              │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ COMPUTE nodes (no LLM reasoning):                                            │
# │   - Vector search: KNN/ANN similarity search (pure math)                    │
# │   - BM25 search: TF-IDF keyword matching (algorithm)                        │
# │   - Result merging: RRF/score fusion (mathematical)                         │
# │   - Citation formatting: Template-based (string manipulation)               │
# │                                                                              │
# │ AGENT nodes (require LLM reasoning):                                         │
# │   - Query analysis: Understanding intent and context                        │
# │   - Query expansion: Generating synonyms and related terms                  │
# │   - Reranking: Semantic relevance judgment                                  │
# │   - Answer generation: Synthesizing coherent response                       │
# │   - Verification: Checking factual accuracy                                 │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# Pipeline stages:
# 1. Query Analysis - Understand user intent (AGENT)
# 2. Query Expansion - Generate search variants (AGENT)
# 3. Hybrid Search - Dense + sparse retrieval (COMPUTE)
# 4. Reranking - Semantic relevance scoring (AGENT)
# 5. Answer Generation - Synthesize with citations (AGENT)
# 6. Verification - Fact-check against sources (AGENT)

workflows:
  rag_query:
    description: "Answer questions using retrieved context with citations"

    metadata:
      version: "1.0"
      author: "victor"
      vertical: rag

    # =========================================================================
    # SERVICE DEPENDENCIES
    # =========================================================================
    # Query workflow requires read access to services created by ingest workflow
    services:
      # Project database for metadata lookups
      project_db:
        type: sqlite
        config:
          path: $ctx.project_dir/.victor/project.db
          mode: readonly              # Query workflow only reads
        lifecycle:
          start: auto
          cleanup: none               # Don't modify on cleanup

      # Vector store for similarity search
      vector_store:
        type: lancedb
        config:
          path: $ctx.project_dir/.victor/vectors
        lifecycle:
          start: auto
          cleanup: none

    nodes:
      # =====================================================================
      # Stage 1: Query Understanding
      # =====================================================================
      - id: analyze_query
        type: agent
        name: "Analyze Query"
        role: analyst
        goal: |
          Analyze the user query to understand:
          1. Query type (factual, analytical, comparative, procedural)
          2. Key concepts and entities
          3. Implicit constraints (time range, domain)
          4. Required depth of answer
          5. Whether multi-hop reasoning is needed

          Output structured analysis for search optimization.
        tool_budget: 10
        llm_config:
          temperature: 0.2
          model_hint: claude-3-haiku
        output: query_analysis
        next: [expand_query]

      - id: expand_query
        type: agent
        name: "Query Expansion"
        role: executor
        goal: |
          Expand the query to improve retrieval:

          Techniques:
          - Synonym expansion
          - Acronym resolution
          - Concept decomposition (for multi-hop)
          - Alternative phrasings

          Generate 3-5 query variants ranked by expected relevance.
        tool_budget: 10
        llm_config:
          temperature: 0.3
        input_mapping:
          analysis: query_analysis
          original_query: $ctx.user_query
        output: expanded_queries
        next: [parallel_search]

      # =====================================================================
      # Stage 2: Hybrid Search (Parallel)
      # =====================================================================
      - id: parallel_search
        type: parallel
        name: "Hybrid Search"
        parallel_nodes: [dense_search, sparse_search, entity_search]
        join_strategy: all
        next: [merge_results]

      # COMPUTE: Vector search is KNN/ANN algorithm (HNSW, IVF)
      # Pure mathematical similarity: cosine/dot product/euclidean
      # No LLM reasoning - returns top-k nearest neighbors
      # Execution: in-process (LanceDB) or network (Pinecone/Qdrant)
      # Constraints: NETWORK for cloud stores, FILESYSTEM for local
      - id: dense_search
        type: compute
        name: "Dense Vector Search"
        handler: retry_with_backoff  # Handles rate limits/timeouts
        tools: []  # Uses vector store client directly
        inputs:
          queries: $ctx.expanded_queries
          index_name: $ctx.index_name
          top_k: 20                               # Retrieve 20 candidates
          similarity_threshold: $ctx.similarity_threshold  # Default: 0.7
          # metric: cosine | dot_product | euclidean
        output: dense_results
        constraints:
          llm_allowed: false
          network_allowed: true   # Required for cloud vector stores
          write_allowed: false    # Read-only search
          timeout: 30

      # COMPUTE: BM25/TF-IDF is statistical keyword matching
      # Okapi BM25 algorithm - no semantic understanding
      # Complements dense search for exact keyword matches
      # Execution: in-process (tantivy/whoosh) or service
      # Constraints: SANDBOXED - pure text matching
      - id: sparse_search
        type: compute
        name: "Sparse BM25 Search"
        tools: []
        inputs:
          queries: $ctx.expanded_queries
          index_name: $ctx.sparse_index  # Separate BM25 index
          top_k: 20
          # BM25 parameters:
          # k1: 1.2 (term frequency saturation)
          # b: 0.75 (document length normalization)
        output: sparse_results
        constraints:
          llm_allowed: false
          network_allowed: false  # Local index search
          filesystem_allowed: true  # Index on disk
          timeout: 15

      # COMPUTE: Entity search is structured query on metadata
      # Filters by extracted entity values (ORG, PERSON, etc.)
      # No reasoning - exact/fuzzy match on entity fields
      # Execution: in-process (SQLite FTS or vector store filter)
      - id: entity_search
        type: compute
        name: "Entity-Based Search"
        tools: []
        inputs:
          entities: $ctx.query_analysis.entities
          index_name: $ctx.entity_index
          top_k: 10
          # match_type: exact | fuzzy | prefix
        output: entity_results
        constraints:
          llm_allowed: false
          network_allowed: false
          timeout: 15

      # COMPUTE: Result fusion is mathematical score aggregation
      # Reciprocal Rank Fusion (RRF) or weighted combination
      # Formula: score = sum(1 / (k + rank)) across result sets
      # Execution: in-process (pure Python)
      # Constraints: SANDBOXED - operates on in-memory result sets
      - id: merge_results
        type: compute
        name: "Merge and Deduplicate Results"
        tools: []
        inputs:
          dense: $ctx.dense_results
          sparse: $ctx.sparse_results
          entity: $ctx.entity_results
          fusion_method: reciprocal_rank  # RRF with k=60
          # Alternative: weighted_sum (weights: [0.5, 0.3, 0.2])
          dedupe_by: chunk_id
        output: merged_results
        constraints:
          llm_allowed: false
          network_allowed: false
          filesystem_allowed: false
          timeout: 10
        next: [check_results]

      # =====================================================================
      # Stage 3: Result Validation
      # =====================================================================
      - id: check_results
        type: condition
        name: "Check Search Results"
        condition: "result_count >= 3"
        branches:
          "true": rerank
          "false": fallback_search

      # COMPUTE: Fallback search with lower threshold
      # Same vector search algorithm, just broader parameters
      # Execution: same as dense_search
      - id: fallback_search
        type: compute
        name: "Fallback Broad Search"
        tools: []
        inputs:
          query: $ctx.user_query
          index_name: $ctx.index_name
          top_k: 50                    # More results
          similarity_threshold: 0.3   # Lower threshold
        output: fallback_results
        constraints:
          llm_allowed: false
          network_allowed: true
          timeout: 30
        next: [check_fallback]

      - id: check_fallback
        type: condition
        name: "Check Fallback Results"
        condition: "fallback_count >= 1"
        branches:
          "true": rerank
          "false": no_context_answer

      - id: no_context_answer
        type: agent
        name: "Answer Without Context"
        role: executor
        goal: |
          No relevant context was found in the knowledge base.

          Options:
          1. Answer from general knowledge (if appropriate)
          2. Explain what information is missing
          3. Suggest reformulating the query

          Be transparent about lack of supporting evidence.
        tool_budget: 10
        llm_config:
          temperature: 0.3
        output: fallback_answer
        next: [complete_no_context]

      - id: complete_no_context
        type: transform
        name: "Complete (No Context)"
        transform: |
          status = "completed"
          answer = fallback_answer
          confidence = "low"
          sources = []

      # =====================================================================
      # Stage 4: Reranking
      # =====================================================================
      - id: rerank
        type: agent
        name: "Rerank and Filter Results"
        role: analyst
        goal: |
          Rerank retrieved chunks for relevance:

          Consider:
          1. Semantic similarity to query
          2. Factual completeness
          3. Recency (if time-sensitive)
          4. Source authority
          5. Context overlap (reduce redundancy)

          Select top 5-10 most relevant chunks.
        tool_budget: 15
        llm_config:
          temperature: 0.2
        input_mapping:
          results: merged_results
          query: $ctx.user_query
          analysis: query_analysis
        output: ranked_results
        next: [check_coverage]

      # =====================================================================
      # Stage 5: Context Sufficiency Check
      # =====================================================================
      - id: check_coverage
        type: agent
        name: "Check Context Coverage"
        role: analyst
        goal: |
          Evaluate if the retrieved context can answer the query:

          Assessment criteria:
          1. Does context contain the answer? (yes/partial/no)
          2. Are there missing pieces needed?
          3. Confidence level (high/medium/low)

          If partial, identify what's missing.
        tool_budget: 10
        llm_config:
          temperature: 0.2
        input_mapping:
          context: ranked_results
          query: $ctx.user_query
        output: coverage_assessment
        next: [coverage_decision]

      - id: coverage_decision
        type: condition
        name: "Coverage Decision"
        condition: "coverage_assessment.has_answer"
        branches:
          "true": generate_answer
          "false": handle_partial

      - id: handle_partial
        type: condition
        name: "Check Confidence Level"
        condition: "coverage_assessment.confidence >= 'medium'"
        branches:
          "true": generate_answer
          "false": request_clarification

      - id: request_clarification
        type: hitl
        name: "Request Query Clarification"
        hitl_type: input
        prompt: |
          ## Additional Information Needed

          **Original Query:** {user_query}

          **Available Context:** The knowledge base contains partial information but
          cannot fully answer your question.

          **Missing:** {missing_information}

          Would you like to:
          1. Proceed with partial answer
          2. Rephrase your question
          3. Specify which aspect to focus on
        context_keys:
          - user_query
          - missing_information
        timeout: 300
        fallback: continue
        next: [handle_clarification]

      - id: handle_clarification
        type: condition
        name: "Handle Clarification"
        condition: "clarification_choice"
        branches:
          "proceed_partial": generate_answer
          "rephrase": analyze_query
          "focus": generate_answer

      # =====================================================================
      # Stage 6: Answer Generation
      # =====================================================================
      - id: generate_answer
        type: agent
        name: "Generate Answer with Citations"
        role: writer
        goal: |
          Generate a comprehensive answer using the retrieved context.

          Requirements:
          1. Answer the specific question asked
          2. Support claims with inline citations [1], [2], etc.
          3. Synthesize information from multiple sources
          4. Acknowledge uncertainty where appropriate
          5. Stay factual - don't hallucinate beyond context

          Format:
          - Clear, well-structured prose
          - Citations linked to source chunks
          - Confidence indicator at end
        tool_budget: 25
        llm_config:
          temperature: 0.3
          max_tokens: 2000
        input_mapping:
          context: ranked_results
          query: $ctx.user_query
          analysis: query_analysis
        output: draft_answer
        next: [verify_answer]

      # =====================================================================
      # Stage 7: Answer Verification
      # =====================================================================
      - id: verify_answer
        type: agent
        name: "Verify Answer Accuracy"
        role: reviewer
        goal: |
          Verify the generated answer:

          Checks:
          1. All claims are supported by cited context
          2. No hallucinated facts
          3. Citations are accurate (quote matches source)
          4. Logical coherence
          5. Addresses the query completely

          Flag any issues for revision.
        tool_budget: 15
        llm_config:
          temperature: 0.1
        input_mapping:
          answer: draft_answer
          context: ranked_results
          query: $ctx.user_query
        output: verification
        next: [check_verification]

      - id: check_verification
        type: condition
        name: "Check Verification"
        condition: "verification.passed"
        branches:
          "true": format_response
          "false": revise_answer

      - id: revise_answer
        type: condition
        name: "Check Revision Count"
        condition: "revision_count < 2"
        branches:
          "true": apply_revision
          "false": format_response

      - id: apply_revision
        type: agent
        name: "Revise Answer"
        role: executor
        goal: |
          Revise the answer based on verification feedback:
          {verification_issues}

          Fix identified issues while maintaining accuracy.
        tool_budget: 15
        llm_config:
          temperature: 0.2
        input_mapping:
          current_answer: draft_answer
          issues: verification.issues
          context: ranked_results
        output: revised_answer
        next: [verify_answer]

      # =====================================================================
      # Stage 8: Response Formatting
      # =====================================================================
      - id: format_response
        type: transform
        name: "Format Final Response"
        transform: |
          final_answer = draft_answer if verification.passed else revised_answer
          sources = [
            {
              "id": i + 1,
              "title": chunk.source_title,
              "url": chunk.source_url,
              "excerpt": chunk.text[:200],
            }
            for i, chunk in enumerate(ranked_results[:10])
          ]
          confidence = coverage_assessment.confidence
        next: [complete]

      - id: complete
        type: transform
        name: "Query Complete"
        transform: |
          status = "completed"
          query_time = elapsed_time()


  # =========================================================================
  # Multi-Turn Conversation
  # =========================================================================
  conversation:
    description: "Multi-turn RAG conversation with context persistence"

    metadata:
      vertical: rag

    nodes:
      - id: load_history
        type: compute
        name: "Load Conversation History"
        tools: [read]
        inputs:
          session_id: $ctx.session_id
        output: history
        constraints:
          llm_allowed: false
          timeout: 10
        next: [contextualize]

      - id: contextualize
        type: agent
        name: "Contextualize Query"
        role: analyst
        goal: |
          Understand the current query in context of conversation history:

          1. Resolve pronouns and references
          2. Inherit constraints from prior turns
          3. Identify if this is a follow-up or new topic
          4. Expand query with conversation context
        tool_budget: 10
        llm_config:
          temperature: 0.2
        input_mapping:
          current_query: $ctx.user_query
          history: history
        output: contextualized_query
        next: [parallel_search]

      - id: parallel_search
        type: parallel
        name: "Search with Context"
        parallel_nodes: [dense_search, sparse_search]
        join_strategy: all
        next: [generate]

      - id: dense_search
        type: compute
        name: "Dense Search"
        tools: [shell]
        inputs:
          query: $ctx.contextualized_query
          top_k: 15
        output: dense
        constraints:
          llm_allowed: false
          timeout: 30

      - id: sparse_search
        type: compute
        name: "Sparse Search"
        tools: [shell]
        inputs:
          query: $ctx.contextualized_query
          top_k: 15
        output: sparse
        constraints:
          llm_allowed: false
          timeout: 15

      - id: generate
        type: agent
        name: "Generate Contextual Answer"
        role: writer
        goal: |
          Generate answer considering conversation context.

          Maintain consistency with previous answers.
          Reference prior discussion where relevant.
        tool_budget: 20
        llm_config:
          temperature: 0.3
        input_mapping:
          context: merged(dense, sparse)
          query: contextualized_query
          history: history
        output: answer
        next: [save_turn]

      - id: save_turn
        type: compute
        name: "Save Conversation Turn"
        tools: [write]
        inputs:
          session_id: $ctx.session_id
          turn:
            query: $ctx.user_query
            answer: $ctx.answer
            sources: $ctx.sources
        constraints:
          llm_allowed: false
          write_allowed: true
          timeout: 10
        next: [complete]

      - id: complete
        type: transform
        name: "Turn Complete"
        transform: |
          status = "completed"


  # =========================================================================
  # Agentic RAG with Tool Use
  # =========================================================================
  agentic_rag:
    description: "RAG with agentic reasoning and tool use"

    metadata:
      vertical: rag

    nodes:
      - id: plan_approach
        type: agent
        name: "Plan Query Approach"
        role: planner
        goal: |
          Analyze the query and plan how to answer it:

          1. Identify sub-questions to answer
          2. Determine which tools/searches are needed
          3. Plan reasoning steps
          4. Estimate confidence achievable

          Complex queries may need multiple search rounds.
        tool_budget: 10
        llm_config:
          temperature: 0.3
        output: plan
        next: [execute_plan]

      - id: execute_plan
        type: agent
        name: "Execute Search Plan"
        role: executor
        goal: |
          Execute the search plan step by step.

          For each sub-question:
          1. Search relevant sources
          2. Extract key information
          3. Note gaps or contradictions

          Use tools as needed.
        tool_budget: 40
        tools: [web_search, read, shell]
        llm_config:
          temperature: 0.2
        input_mapping:
          plan: plan
        output: execution_results
        next: [synthesize]

      - id: synthesize
        type: agent
        name: "Synthesize Findings"
        role: analyst
        goal: |
          Synthesize findings from all search steps:

          1. Combine evidence from multiple sources
          2. Resolve contradictions
          3. Fill gaps with reasoning
          4. Assess overall confidence
        tool_budget: 20
        llm_config:
          temperature: 0.4
        input_mapping:
          results: execution_results
          original_query: $ctx.user_query
        output: synthesis
        next: [generate_response]

      - id: generate_response
        type: agent
        name: "Generate Final Response"
        role: writer
        goal: |
          Generate comprehensive response from synthesis.

          Include:
          - Clear answer
          - Supporting evidence
          - Confidence assessment
          - Limitations/caveats
        tool_budget: 15
        llm_config:
          temperature: 0.3
          max_tokens: 2000
        output: final_response
        next: [complete]

      - id: complete
        type: transform
        name: "Complete"
        transform: |
          status = "completed"


  # =========================================================================
  # Index Maintenance Workflow
  # =========================================================================
  maintenance:
    description: "Index maintenance and optimization"

    metadata:
      version: "1.0"
      author: "victor"
      vertical: rag

    # =========================================================================
    # SERVICE DEPENDENCIES
    # =========================================================================
    # Maintenance workflow needs write access to clean/optimize stores
    services:
      project_db:
        type: sqlite
        config:
          path: $ctx.project_dir/.victor/project.db
          mode: readwrite           # Need write for cleanup
        lifecycle:
          start: auto
          cleanup: vacuum           # Run VACUUM after maintenance

      vector_store:
        type: lancedb
        config:
          path: $ctx.project_dir/.victor/vectors
        lifecycle:
          start: auto
          cleanup: compact          # Compact after deletions

    nodes:
      # =====================================================================
      # Stage 1: Analyze Index State (Compute - pure diagnostics)
      # =====================================================================
      # COMPUTE: Index analysis is database queries + file stats
      # - Vector count: SELECT COUNT(*) or collection.count()
      # - Index size: filesystem stat on index files
      # - Fragmentation: ratio of active vs total space
      # No LLM reasoning - pure metric collection
      # Execution: in-process (database queries)
      # Constraints: FILESYSTEM read for stats
      - id: analyze_index
        type: compute
        name: "Analyze Index State"
        handler: index_analyzer
        tools: []  # Uses database/store clients
        inputs:
          index_name: $ctx.index_name
          checks:
            statistics:
              - vector_count        # COUNT(*) on vectors
              - document_count      # Distinct source docs
              - index_size          # Bytes on disk
              - embedding_dimensions  # Vector dim check
              - last_updated        # Max ingestion timestamp
            health:
              - fragmentation_level  # (total - active) / total
              - orphaned_entries    # Vectors without metadata
              - stale_documents     # Modified since ingest
              - duplicate_content   # Near-duplicate chunks
            performance:
              - avg_query_latency   # From metrics table
              - search_quality_score  # Hit rate tracking
              - index_query_ratio   # Queries per document
        output: index_analysis
        constraints:
          llm_allowed: false
          network_allowed: false  # Local analysis only
          write_allowed: false    # Read-only analysis
          timeout: 120
        next: [check_issues]

      - id: check_issues
        type: condition
        name: "Check for Issues"
        condition: "has_issues"
        branches:
          "true": cleanup
          "false": check_optimization

      - id: check_optimization
        type: condition
        name: "Check Optimization Needed"
        condition: "needs_optimization"
        branches:
          "true": optimize
          "false": generate_report

      # =====================================================================
      # Stage 2: Cleanup Operations (Compute - deterministic maintenance)
      # =====================================================================
      # COMPUTE: Cleanup is DELETE operations on database
      # - Orphaned: DELETE WHERE source_id NOT IN (SELECT id FROM documents)
      # - Stale: DELETE WHERE ingested_at < (NOW - retention_days)
      # - Duplicates: DELETE keeping MAX(quality_score) per hash
      # No LLM reasoning - rule-based deletions
      # Execution: in-process (database transactions)
      # Constraints: WRITE REQUIRED, FILESYSTEM for local stores
      - id: cleanup
        type: compute
        name: "Cleanup Index"
        handler: index_cleaner
        tools: []
        inputs:
          index_name: $ctx.index_name
          analysis: $ctx.index_analysis
          operations:
            remove_orphaned:
              - vectors_without_source  # FK violation check
              - partial_entries         # Incomplete ingestion
              - corrupted_entries       # Invalid vector data
            handle_stale:
              retention_days: $ctx.retention_days  # Default: 365
              remove_deleted_sources: true         # Source file removed
            deduplicate:
              similarity_threshold: 0.95  # Near-duplicates
              keep_strategy: highest_quality  # Keep best quality score
          log_deletions: true  # Write deletion log for audit
        output: cleanup_result
        constraints:
          llm_allowed: false
          network_allowed: false  # Local store only
          write_allowed: true     # REQUIRED for deletions
          timeout: 300

      # =====================================================================
      # Stage 3: Optimization
      # =====================================================================
      # COMPUTE: Optimization is database maintenance operations
      # - Compact: Reclaim space from deleted entries
      # - Rebuild HNSW: Rebalance graph index for query perf
      # - Update stats: Refresh query planner statistics
      # Execution: in-process (database operations)
      # Constraints: WRITE + FILESYSTEM, may lock index
      - id: optimize
        type: compute
        name: "Optimize Index"
        handler: retry_with_backoff  # Handles lock contention
        tools: []
        inputs:
          index_name: $ctx.index_name
          operations:
            - compact_vectors    # LanceDB: compact(), Chroma: persist()
            - rebuild_hnsw       # Rebuild graph for better perf
            - update_statistics  # ANALYZE in SQLite
        output: optimization_result
        constraints:
          llm_allowed: false
          network_allowed: false
          write_allowed: true    # REQUIRED for optimization
          timeout: 600           # May be slow for large indices

      # COMPUTE: Verification is benchmark query execution
      # Runs sample queries to measure latency/quality
      # Execution: in-process (timed queries)
      # Constraints: READ-ONLY verification
      - id: verify_optimization
        type: compute
        name: "Verify Optimization"
        tools: []
        inputs:
          index_name: $ctx.index_name
          verify:
            - query_latency    # P50/P95 of sample queries
            - search_quality   # MRR on golden set
          sample_queries: $ctx.golden_queries  # Optional: known good queries
        output: verification
        constraints:
          llm_allowed: false
          network_allowed: false
          write_allowed: false   # Read-only benchmark
          timeout: 60
        next: [generate_report]

      # =====================================================================
      # Stage 4: Generate Report (Compute - templated report)
      # =====================================================================
      # COMPUTE: Report uses Jinja2 template - no LLM
      # Fills template with metrics and comparison tables
      # Execution: in-process (template rendering)
      # Constraints: FILESYSTEM for output file
      - id: generate_report
        type: compute
        name: "Generate Maintenance Report"
        handler: report_generator
        tools: [write]
        inputs:
          template: maintenance_summary  # templates/maintenance_summary.md.j2
          output_path: $ctx.report_path  # Default: ./maintenance_report.md
          sections:
            health_summary:
              current_status: $ctx.index_analysis.status
              issues_found: $ctx.index_analysis.issues
              issues_fixed: $ctx.cleanup_result.fixed_count
              optimization_applied: $ctx.optimization_result.operations
            statistics:
              before: $ctx.index_analysis.metrics
              after: $ctx.verification.metrics
              space_reclaimed: $ctx.cleanup_result.space_freed
              performance_improvement: $ctx.verification.latency_improvement
            recommendations:
              retention_policy: $ctx.recommended_retention
              next_maintenance: $ctx.next_scheduled
              capacity_notes: $ctx.capacity_forecast
        output: report
        constraints:
          llm_allowed: false
          network_allowed: false  # Local file write only
          write_allowed: true     # REQUIRED for report output
          timeout: 30
        next: [complete]

      - id: complete
        type: transform
        name: "Maintenance Complete"
        transform: |
          status = "completed"
          maintenance_time = elapsed_time()
          next_scheduled = calculate_next_maintenance()
