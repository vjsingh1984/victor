# EDA Pipeline Demo Workflow
# ============================
# Showcases 80% of regular agentic AI workflow patterns:
# 1. Sequential tool chaining
# 2. Parallel execution (fan-out/fan-in)
# 3. Conditional branching (decision nodes)
# 4. Agent loops (retry, refinement)
# 5. Human-in-the-loop gates
# 6. Error recovery paths
# 7. Hybrid ComputeNode + AgentNode execution

workflows:
  eda_pipeline:
    description: "Full EDA pipeline demonstrating core agentic patterns"

    metadata:
      version: "2.0"
      author: "victor"
      vertical: dataanalysis

    batch_config:
      batch_size: 5
      max_concurrent: 3
      retry_strategy: end_of_batch
      max_retries: 2

    temporal_context:
      as_of_date: $ctx.analysis_date
      lookback_periods: 4
      period_type: quarters

    nodes:
      # =====================================================================
      # PATTERN 1: Sequential Tool Chaining
      # Load -> Validate -> Process
      # =====================================================================
      - id: load_data
        type: compute
        name: "Load Dataset"
        tools: [read, shell]
        inputs:
          file_path: $ctx.data_file
          format: $ctx.file_format
        output: raw_data
        constraints:
          llm_allowed: false
          max_cost_tier: FREE
          timeout: 30
        next: [validate]

      - id: validate
        type: compute
        name: "Validate Data Quality"
        handler: data_transform
        inputs:
          data: $ctx.raw_data
          validations:
            - check_dtypes
            - check_missing
            - check_duplicates
        output: validation_result
        constraints:
          llm_allowed: false
          max_cost_tier: FREE
          timeout: 60
        next: [check_quality]

      # =====================================================================
      # PATTERN 2: Conditional Branching
      # Route based on data quality score
      # =====================================================================
      - id: check_quality
        type: condition
        name: "Check Data Quality"
        condition: "quality_score >= 0.8"
        branches:
          "true": parallel_stats       # Good quality -> proceed
          "false": data_cleanup        # Poor quality -> cleanup first
          "default": parallel_stats
        next: []

      - id: data_cleanup
        type: agent
        name: "Clean Data Issues"
        role: executor
        goal: |
          Fix data quality issues identified in the validation:
          - Handle missing values (impute or drop)
          - Remove duplicates
          - Fix data type issues
          - Normalize outliers if needed
        tool_budget: 20
        llm_config:
          temperature: 0.2
          model_hint: claude-3-sonnet
        input_mapping:
          issues: validation_result
          data: raw_data
        output: cleaned_data
        next: [validate_cleanup]

      # =====================================================================
      # PATTERN 3: Agent Loop (Retry with Refinement)
      # Re-validate after cleanup, loop back if still failing
      # =====================================================================
      - id: validate_cleanup
        type: compute
        name: "Re-validate Cleaned Data"
        handler: data_transform
        inputs:
          data: $ctx.cleaned_data
          validations:
            - check_dtypes
            - check_missing
        output: cleanup_validation
        constraints:
          llm_allowed: false
          timeout: 30
        next: [cleanup_loop_check]

      - id: cleanup_loop_check
        type: condition
        name: "Check Cleanup Success"
        condition: "cleanup_attempts < 3 and cleanup_quality < 0.7"
        branches:
          "true": data_cleanup          # Loop back for more cleanup
          "false": parallel_stats       # Proceed with analysis
          "default": parallel_stats

      # =====================================================================
      # PATTERN 4: Parallel Execution (Fan-out)
      # Multiple independent computations in parallel
      # =====================================================================
      - id: parallel_stats
        type: parallel
        name: "Compute Statistics in Parallel"
        parallel_nodes: [basic_stats, correlations, distributions, anomaly_detection]
        join_strategy: all
        next: [aggregate_stats]

      - id: basic_stats
        type: compute
        name: "Basic Statistics"
        tools: [shell]
        inputs:
          data: $ctx.raw_data
        output: statistics
        constraints:
          llm_allowed: false
          max_cost_tier: FREE
          timeout: 60

      - id: correlations
        type: compute
        name: "Correlation Matrix"
        handler: parallel_tools
        tools: [shell]
        inputs:
          data: $ctx.raw_data
          method: pearson
        output: correlation_matrix
        constraints:
          llm_allowed: false
          timeout: 60

      - id: distributions
        type: compute
        name: "Distribution Analysis"
        tools: [shell]
        inputs:
          data: $ctx.raw_data
          bins: 50
        output: distribution_data
        constraints:
          llm_allowed: false
          timeout: 60

      - id: anomaly_detection
        type: compute
        name: "Detect Anomalies"
        tools: [shell]
        inputs:
          data: $ctx.raw_data
          method: isolation_forest
          contamination: 0.05
        output: anomalies
        constraints:
          llm_allowed: false
          timeout: 120

      # =====================================================================
      # PATTERN 5: Fan-in (Aggregate parallel results)
      # =====================================================================
      - id: aggregate_stats
        type: transform
        name: "Aggregate Results"
        transform: |
          analysis_results = merge(
            statistics,
            correlation_matrix,
            distribution_data,
            anomalies
          )
        next: [analyze]

      # =====================================================================
      # PATTERN 6: LLM Agent Analysis with Tool Use
      # Agent decides what additional analysis is needed
      # =====================================================================
      - id: analyze
        type: agent
        name: "AI-Powered Analysis"
        role: analyst
        goal: |
          Analyze the computed statistics and:
          1. Identify key patterns and trends
          2. Flag potential data quality concerns
          3. Highlight interesting correlations
          4. Explain anomalies detected
          5. Recommend follow-up analyses

          Use tools to explore specific aspects if needed.
        tool_budget: 25
        tools: [shell, read, grep]
        llm_config:
          temperature: 0.4
          model_hint: claude-3-sonnet
        input_mapping:
          stats: statistics
          corr: correlation_matrix
          dist: distribution_data
          anomalies: anomalies
        output: insights
        next: [check_confidence]

      # =====================================================================
      # PATTERN 7: Confidence-Based Branching
      # Route based on analysis confidence
      # =====================================================================
      - id: check_confidence
        type: condition
        name: "Check Analysis Confidence"
        condition: "analysis_confidence"
        branches:
          "high": visualize
          "medium": deep_dive
          "low": human_assist
          "default": visualize

      - id: deep_dive
        type: agent
        name: "Deep Dive Analysis"
        role: researcher
        goal: |
          Perform deeper analysis on areas of uncertainty:
          - Investigate unclear correlations
          - Analyze edge cases
          - Validate hypotheses with statistical tests
        tool_budget: 30
        llm_config:
          temperature: 0.3
        input_mapping:
          previous_analysis: insights
          data: raw_data
        output: deep_insights
        next: [merge_insights]

      - id: human_assist
        type: hitl
        name: "Request Human Guidance"
        hitl_type: input
        prompt: |
          The AI analysis has low confidence on several aspects:

          **Uncertain Areas:**
          {uncertainty_areas}

          Please provide guidance on:
          1. Domain-specific context that might explain patterns
          2. Known data issues to consider
          3. Priority areas to focus on

          Enter your guidance:
        context_keys:
          - uncertainty_areas
          - insights
        timeout: 1800
        fallback: continue
        next: [refine_analysis]

      - id: refine_analysis
        type: agent
        name: "Refine with Human Input"
        role: analyst
        goal: |
          Refine the analysis using human-provided guidance:
          {human_guidance}

          Update insights considering this domain knowledge.
        tool_budget: 15
        input_mapping:
          original_insights: insights
          guidance: human_guidance
        output: refined_insights
        next: [merge_insights]

      - id: merge_insights
        type: transform
        name: "Merge All Insights"
        transform: |
          final_insights = merge(insights, deep_insights, refined_insights)
        next: [visualize]

      # =====================================================================
      # PATTERN 8: Parallel Visualization Generation
      # =====================================================================
      - id: visualize
        type: parallel
        name: "Generate Visualizations"
        parallel_nodes: [summary_charts, correlation_heatmap, anomaly_plots]
        join_strategy: all
        next: [review]

      - id: summary_charts
        type: compute
        name: "Summary Charts"
        tools: [shell]
        inputs:
          data: $ctx.raw_data
          stats: $ctx.statistics
          output_dir: $ctx.output_dir
        output: chart_paths
        constraints:
          write_allowed: true
          timeout: 120

      - id: correlation_heatmap
        type: compute
        name: "Correlation Heatmap"
        tools: [shell]
        inputs:
          correlations: $ctx.correlation_matrix
          output_path: $ctx.output_dir
        output: heatmap_path
        constraints:
          write_allowed: true
          timeout: 60

      - id: anomaly_plots
        type: compute
        name: "Anomaly Visualizations"
        tools: [shell]
        inputs:
          anomalies: $ctx.anomalies
          data: $ctx.raw_data
          output_dir: $ctx.output_dir
        output: anomaly_chart_paths
        constraints:
          write_allowed: true
          timeout: 90

      # =====================================================================
      # PATTERN 9: Human-in-the-Loop Approval Gate
      # =====================================================================
      - id: review
        type: hitl
        name: "Human Review"
        hitl_type: approval
        prompt: |
          ## EDA Analysis Complete

          **Key Insights:**
          {final_insights}

          **Visualizations Generated:**
          - Summary Charts: {chart_paths}
          - Correlation Heatmap: {heatmap_path}
          - Anomaly Plots: {anomaly_chart_paths}

          Do you approve generating the final report?
        context_keys:
          - final_insights
          - chart_paths
          - heatmap_path
          - anomaly_chart_paths
        choices:
          - "Approve and generate report"
          - "Request revisions"
          - "Skip report generation"
        timeout: 900
        fallback: continue
        next: [check_approval]

      - id: check_approval
        type: condition
        name: "Check Approval Decision"
        condition: "approval_decision"
        branches:
          "approve": report
          "revisions": revision_loop
          "skip": end
          "default": report

      # =====================================================================
      # PATTERN 10: Revision Loop (Iterative Refinement)
      # =====================================================================
      - id: revision_loop
        type: agent
        name: "Apply Revisions"
        role: executor
        goal: |
          Apply the requested revisions:
          {revision_feedback}

          Update the analysis and visualizations accordingly.
        tool_budget: 20
        tools: [shell, write]
        input_mapping:
          feedback: revision_feedback
          current_insights: final_insights
        output: revised_insights
        next: [increment_revision]

      - id: increment_revision
        type: transform
        name: "Track Revision Count"
        transform: "revision_count = revision_count + 1"
        next: [revision_limit_check]

      - id: revision_limit_check
        type: condition
        name: "Check Revision Limit"
        condition: "revision_count < 3"
        branches:
          "true": review           # Loop back for re-review
          "false": report          # Max revisions, proceed to report
          "default": report

      # =====================================================================
      # PATTERN 11: Final Report Generation
      # =====================================================================
      - id: report
        type: agent
        name: "Generate Final Report"
        role: writer
        goal: |
          Generate a comprehensive EDA report including:
          1. Executive summary
          2. Data quality assessment
          3. Key findings and insights
          4. Visualization descriptions
          5. Recommendations for next steps
          6. Appendix with technical details

          Format as professional markdown document.
        tool_budget: 15
        tools: [write]
        llm_config:
          temperature: 0.5
          max_tokens: 8000
        input_mapping:
          insights: final_insights
          stats: statistics
          validation: validation_result
          charts: chart_paths
        output: final_report
        next: [end]

      - id: end
        type: transform
        name: "Workflow Complete"
        transform: |
          workflow_status = "completed"
          completion_time = current_timestamp()


  # =========================================================================
  # Lightweight EDA - Quick Analysis Mode
  # =========================================================================
  eda_quick:
    description: "Quick EDA with minimal LLM usage"

    metadata:
      vertical: dataanalysis

    nodes:
      - id: quick_stats
        type: compute
        name: "Quick Statistics"
        handler: parallel_tools
        tools: [shell]
        inputs:
          file_path: $ctx.data_file
        output: stats
        constraints:
          llm_allowed: false
          max_cost_tier: FREE
          timeout: 45
        next: [quick_check]

      - id: quick_check
        type: condition
        name: "Data Size Check"
        condition: "row_count > 10000"
        branches:
          "true": sample_analysis
          "false": full_quick_analysis

      - id: sample_analysis
        type: compute
        name: "Sample-based Analysis"
        tools: [shell]
        inputs:
          sample_size: 5000
        output: sampled_stats
        constraints:
          llm_allowed: false
          timeout: 30
        next: [quick_insights]

      - id: full_quick_analysis
        type: compute
        name: "Full Quick Analysis"
        tools: [shell]
        output: full_stats
        constraints:
          llm_allowed: false
          timeout: 60
        next: [quick_insights]

      - id: quick_insights
        type: agent
        name: "Quick Insights"
        role: analyst
        goal: "Provide 5 key insights from the statistics. Be concise."
        tool_budget: 5
        llm_config:
          temperature: 0.2
          model_hint: claude-3-haiku
        output: insights
