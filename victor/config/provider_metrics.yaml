---
# Provider Metrics Capabilities Configuration
# SINGLE SOURCE OF TRUTH for metrics capabilities per provider
#
# This file defines what metrics each provider supports and their pricing.
# Users can override these settings in ~/.victor/profiles.yaml under provider_metrics:
#
# Resolution Order:
#   1. User overrides (profiles.yaml provider_metrics)
#   2. Model-specific settings (models section)
#   3. Provider defaults (providers section)
#   4. Global defaults (defaults section)

schema_version: "1.0"

# ==============================================================================
# Default capabilities for all providers
# ==============================================================================
defaults:
  token_metrics:
    prompt_tokens: false
    completion_tokens: false
    total_tokens: false
    cache_read_tokens: false
    cache_write_tokens: false

  cost_metrics:
    enabled: false
    pricing_source: null  # "config" | null (API pricing not supported by any provider)

  estimation:
    fallback_enabled: true
    chars_per_token: 4

# ==============================================================================
# Provider-specific capabilities
# ==============================================================================
providers:
  # ---------------------------------------------------------------------------
  # Anthropic - Full metrics support including cache tokens
  # ---------------------------------------------------------------------------
  anthropic:
    token_metrics:
      prompt_tokens: true
      completion_tokens: true
      total_tokens: true
      cache_read_tokens: true
      cache_write_tokens: true
    cost_metrics:
      enabled: true
      pricing_source: config
      # Pricing per million tokens (USD) - Updated January 2025
      pricing:
        # Claude 3.5 Sonnet (latest)
        claude-3-5-sonnet-20241022:
          input_per_mtok: 3.00
          output_per_mtok: 15.00
          cache_read_per_mtok: 0.30
          cache_write_per_mtok: 3.75
        claude-sonnet-4-20250514:
          input_per_mtok: 3.00
          output_per_mtok: 15.00
          cache_read_per_mtok: 0.30
          cache_write_per_mtok: 3.75
        # Claude 3.5 Haiku
        claude-3-5-haiku-20241022:
          input_per_mtok: 0.80
          output_per_mtok: 4.00
          cache_read_per_mtok: 0.08
          cache_write_per_mtok: 1.00
        # Claude 3 Opus
        claude-3-opus-20240229:
          input_per_mtok: 15.00
          output_per_mtok: 75.00
          cache_read_per_mtok: 1.50
          cache_write_per_mtok: 18.75
        # Claude 3 Haiku (legacy)
        claude-3-haiku-20240307:
          input_per_mtok: 0.25
          output_per_mtok: 1.25
          cache_read_per_mtok: 0.03
          cache_write_per_mtok: 0.30
    estimation:
      fallback_enabled: false  # Always has actual tokens

  # ---------------------------------------------------------------------------
  # OpenAI - Token metrics, no cache tokens
  # ---------------------------------------------------------------------------
  openai:
    token_metrics:
      prompt_tokens: true
      completion_tokens: true
      total_tokens: true
      cache_read_tokens: false
      cache_write_tokens: false
    cost_metrics:
      enabled: true
      pricing_source: config
      # Pricing per million tokens (USD) - Updated January 2025
      pricing:
        # GPT-4o series
        gpt-4o:
          input_per_mtok: 2.50
          output_per_mtok: 10.00
        gpt-4o-2024-11-20:
          input_per_mtok: 2.50
          output_per_mtok: 10.00
        gpt-4o-mini:
          input_per_mtok: 0.15
          output_per_mtok: 0.60
        gpt-4o-mini-2024-07-18:
          input_per_mtok: 0.15
          output_per_mtok: 0.60
        # GPT-4 Turbo
        gpt-4-turbo:
          input_per_mtok: 10.00
          output_per_mtok: 30.00
        gpt-4-turbo-preview:
          input_per_mtok: 10.00
          output_per_mtok: 30.00
        # GPT-4 (legacy)
        gpt-4:
          input_per_mtok: 30.00
          output_per_mtok: 60.00
        # GPT-3.5 Turbo
        gpt-3.5-turbo:
          input_per_mtok: 0.50
          output_per_mtok: 1.50
        # o1 reasoning models
        o1:
          input_per_mtok: 15.00
          output_per_mtok: 60.00
        o1-preview:
          input_per_mtok: 15.00
          output_per_mtok: 60.00
        o1-mini:
          input_per_mtok: 3.00
          output_per_mtok: 12.00
    estimation:
      fallback_enabled: false  # Always has actual tokens

  # ---------------------------------------------------------------------------
  # Google (Gemini) - Token metrics
  # ---------------------------------------------------------------------------
  google:
    token_metrics:
      prompt_tokens: true
      completion_tokens: true
      total_tokens: true
      cache_read_tokens: false
      cache_write_tokens: false
    cost_metrics:
      enabled: false  # Users can add pricing in profiles.yaml
    estimation:
      fallback_enabled: false

  # ---------------------------------------------------------------------------
  # Groq - Token metrics (OpenAI-compatible)
  # ---------------------------------------------------------------------------
  groq:
    token_metrics:
      prompt_tokens: true
      completion_tokens: true
      total_tokens: true
      cache_read_tokens: false
      cache_write_tokens: false
    cost_metrics:
      enabled: false  # Groq has generous free tier
    estimation:
      fallback_enabled: false

  # ---------------------------------------------------------------------------
  # Mistral - Token metrics (OpenAI-compatible)
  # ---------------------------------------------------------------------------
  mistral:
    token_metrics:
      prompt_tokens: true
      completion_tokens: true
      total_tokens: true
      cache_read_tokens: false
      cache_write_tokens: false
    cost_metrics:
      enabled: false  # Users can add pricing in profiles.yaml
    estimation:
      fallback_enabled: false

  # ---------------------------------------------------------------------------
  # DeepSeek - Token metrics (OpenAI-compatible)
  # ---------------------------------------------------------------------------
  deepseek:
    token_metrics:
      prompt_tokens: true
      completion_tokens: true
      total_tokens: true
      cache_read_tokens: false
      cache_write_tokens: false
    cost_metrics:
      enabled: false  # Users can add pricing in profiles.yaml
    estimation:
      fallback_enabled: false

  # ---------------------------------------------------------------------------
  # Fireworks - Token metrics (OpenAI-compatible)
  # ---------------------------------------------------------------------------
  fireworks:
    token_metrics:
      prompt_tokens: true
      completion_tokens: true
      total_tokens: true
      cache_read_tokens: false
      cache_write_tokens: false
    cost_metrics:
      enabled: false  # Users can add pricing in profiles.yaml
    estimation:
      fallback_enabled: false

  # ---------------------------------------------------------------------------
  # AWS Bedrock - Token metrics (model-dependent)
  # ---------------------------------------------------------------------------
  bedrock:
    token_metrics:
      prompt_tokens: true
      completion_tokens: true
      total_tokens: true
      cache_read_tokens: false
      cache_write_tokens: false
    cost_metrics:
      enabled: false  # AWS pricing is complex, users can configure
    estimation:
      fallback_enabled: false

  # ---------------------------------------------------------------------------
  # Azure OpenAI - Token metrics
  # ---------------------------------------------------------------------------
  azure_openai:
    token_metrics:
      prompt_tokens: true
      completion_tokens: true
      total_tokens: true
      cache_read_tokens: false
      cache_write_tokens: false
    cost_metrics:
      enabled: false  # Azure pricing varies by deployment
    estimation:
      fallback_enabled: false

  # ---------------------------------------------------------------------------
  # Ollama - Local inference, no token metrics from API
  # ---------------------------------------------------------------------------
  ollama:
    token_metrics:
      prompt_tokens: false
      completion_tokens: false
      total_tokens: false
      cache_read_tokens: false
      cache_write_tokens: false
    cost_metrics:
      enabled: false  # Local, no cost
    estimation:
      fallback_enabled: true
      chars_per_token: 4

  # ---------------------------------------------------------------------------
  # LMStudio - Local inference, no token metrics
  # ---------------------------------------------------------------------------
  lmstudio:
    token_metrics:
      prompt_tokens: false
      completion_tokens: false
      total_tokens: false
      cache_read_tokens: false
      cache_write_tokens: false
    cost_metrics:
      enabled: false  # Local, no cost
    estimation:
      fallback_enabled: true
      chars_per_token: 4

  # ---------------------------------------------------------------------------
  # vLLM - Local/self-hosted, may have token metrics
  # ---------------------------------------------------------------------------
  vllm:
    token_metrics:
      prompt_tokens: true  # vLLM supports OpenAI-compatible usage
      completion_tokens: true
      total_tokens: true
      cache_read_tokens: false
      cache_write_tokens: false
    cost_metrics:
      enabled: false  # Self-hosted, no standard cost
    estimation:
      fallback_enabled: true
      chars_per_token: 4

# ==============================================================================
# Model-specific overrides (fnmatch patterns supported)
# ==============================================================================
models:
  # Example: Override specific model pricing
  # "gpt-4-turbo*":
  #   cost_metrics:
  #     pricing:
  #       input_per_mtok: 10.00
  #       output_per_mtok: 30.00
