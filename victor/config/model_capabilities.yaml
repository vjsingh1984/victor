# Model Capabilities Configuration v0.1.0
#
# MODEL-CENTRIC SCHEMA: Each model is the authoritative source of truth.
# Helper tools can update specific sections for individual models.
#
# Schema:
#   models.<model_pattern>:
#     training:        # What the model was trained to do (provider-independent)
#     providers:       # How each provider enables these capabilities
#     settings:        # Tuning parameters
#
# Resolution order:
#   1. defaults
#   2. provider_defaults.<provider>
#   3. models.<pattern>.training (inherent capabilities)
#   4. models.<pattern>.providers.<provider> (provider-specific overrides)
#   5. models.<pattern>.settings (tuning)

schema_version: "0.1.0"

# =============================================================================
# GLOBAL DEFAULTS
# =============================================================================
defaults:
  # Default capability assumptions for unknown models
  training:
    tool_calling: false
    thinking_mode: false
    code_generation: false
    vision: false
    reasoning: false

  # Default provider support
  provider_support:
    native_tool_calls: false
    streaming_tool_calls: false
    parallel_tool_calls: false
    tool_choice_param: false

  # Default fallback parsing (when native support unavailable)
  fallback:
    json_parsing: true
    xml_parsing: true

  # Default settings
  settings:
    recommended_max_tools: 15
    recommended_tool_budget: 10
    requires_strict_prompting: true
    argument_format: json
    # Exploration multiplier: models that need more "thinking" turns get higher values
    # This multiplies the max_exploration_iterations limit
    exploration_multiplier: 1.0
    # Continuation patience: how many empty turns to allow before forcing completion
    continuation_patience: 3
    # Payload size limits (bytes) - prevents HTTP 413 errors
    # Default is generous; providers with strict limits override this
    max_request_payload_bytes: 10485760  # 10MB default
    # Truncation strategy when payload exceeds limit:
    # - truncate_oldest: Remove oldest messages first
    # - truncate_tool_results: Truncate large tool results
    # - reduce_tools: Reduce number of tools sent
    # - fail: Fail with informative error
    payload_truncation_strategy: truncate_oldest

  # Token-optimized serialization settings
  # Controls how tool outputs are formatted to minimize token usage
  serialization:
    # Preferred format (null = auto-select based on data)
    # Options: json, json_minified, toon, csv, markdown_table, reference_encoded
    preferred_format: null

    # Formats to try for auto-selection (order = preference)
    allowed_formats:
      - toon
      - csv
      - json_minified
      - json

    # Minimum array size to consider tabular formats (TOON/CSV)
    min_array_size_for_tabular: 3

    # Minimum savings threshold to switch from JSON (0.0-1.0)
    min_savings_threshold: 0.20

    # Include format hint for LLM to understand the encoding
    include_format_hint: true

    # Enable reference encoding for repetitive values
    enable_reference_encoding: true

    # Minimum repetition ratio to use references (0.0-1.0)
    min_repetition_for_references: 0.5

    # Maximum nesting depth for compact formats
    max_nesting_for_compact: 1

    # Debug mode - include analysis metadata in output
    debug_mode: false

# =============================================================================
# PROVIDER DEFAULTS
# =============================================================================
# Infrastructure-level capabilities for each provider.
# These define what the provider CAN support, not what individual models do.

provider_defaults:
  # Cloud providers - full native support via their APIs
  anthropic:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 50
    recommended_tool_budget: 20
    # Claude handles all formats well, prefer readability
    serialization:
      preferred_format: null  # auto-select
      include_format_hint: true
      min_savings_threshold: 0.15

  openai:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 50
    recommended_tool_budget: 20

  google:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 15

  xai:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 15

  # Ollama - requires {{ if .Tools }} in Modelfile template
  ollama:
    native_tool_calls: false  # Must be enabled per-model
    streaming_tool_calls: false
    parallel_tool_calls: false
    tool_choice_param: false
    requires_strict_prompting: true
    recommended_max_tools: 20
    recommended_tool_budget: 12

  # LMStudio - llama.cpp enables tools for ALL models (0.3.6+)
  lmstudio:
    native_tool_calls: true
    streaming_tool_calls: false
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 25
    recommended_tool_budget: 15

  # vLLM - guided decoding via Outlines for all models
  vllm:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 15

  # Moonshot AI - OpenAI-compatible API for Kimi K2 models
  moonshot:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 40
    recommended_tool_budget: 20

  # DeepSeek API - OpenAI-compatible with reasoning support
  deepseek:
    native_tool_calls: true  # Only deepseek-chat, not deepseek-reasoner
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 20
    # DeepSeek models need more "thinking" turns, give them higher exploration limit
    exploration_multiplier: 1.5
    continuation_patience: 5

  # Groq Cloud - Ultra-fast inference with free tier
  # NOTE: Groq has strict payload size limits (413 errors) due to LPU architecture
  # Keep max_tools low to prevent payload too large errors
  groqcloud:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 12
    recommended_tool_budget: 15
    # Payload size limit (bytes) - Groq LPU constraint
    max_request_payload_bytes: 4194304  # 4MB - strict Groq limit
    payload_truncation_strategy: truncate_tool_results  # Prefer truncating tool results
    # Aggressive compression to stay within payload limits
    serialization:
      preferred_format: null  # auto-select most compact
      include_format_hint: true
      min_savings_threshold: 0.10  # use compact formats more aggressively
      allowed_formats:
        - csv          # most compact for tabular
        - toon
        - json_minified
        - json

  # Cerebras Cloud - Ultra-fast inference via Wafer Scale Engine
  # Free tier: 30 req/min, 60K-150K tokens/min
  # https://inference-docs.cerebras.ai/
  cerebras:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 20

  # Mistral AI API - Native tool calling support
  # https://docs.mistral.ai/capabilities/function_calling/
  mistral:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 15

  # Together AI - OpenAI-compatible with native tool support
  # https://docs.together.ai/docs/function-calling
  together:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 15

  # OpenRouter - Routes to multiple providers, tool support varies by model
  # https://openrouter.ai/docs#tool-use
  openrouter:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 25
    recommended_tool_budget: 15

  # Fireworks AI - Fast inference with tool calling
  # https://docs.fireworks.ai/guides/function-calling
  fireworks:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 15

  # Azure OpenAI - Same capabilities as OpenAI
  azure:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 50
    recommended_tool_budget: 20

  # AWS Bedrock - Tool support for Claude, Llama, Mistral models
  # https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html
  bedrock:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 40
    recommended_tool_budget: 20

  # Google Vertex AI - Same as Google AI but enterprise
  vertex:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 15

  # HuggingFace Inference API - Model-dependent tool support
  huggingface:
    native_tool_calls: false  # Varies by model, default to false
    streaming_tool_calls: false
    parallel_tool_calls: false
    tool_choice_param: false
    requires_strict_prompting: true
    recommended_max_tools: 15
    recommended_tool_budget: 10

  # Replicate - Model-dependent, mostly for image/audio models
  replicate:
    native_tool_calls: false  # Most models don't support tools
    streaming_tool_calls: false
    parallel_tool_calls: false
    tool_choice_param: false
    requires_strict_prompting: true
    recommended_max_tools: 10
    recommended_tool_budget: 8

# =============================================================================
# MODEL CONFIGURATIONS
# =============================================================================
# Each model pattern is authoritative. Helper tools update specific sections.

models:
  # ---------------------------------------------------------------------------
  # QWEN FAMILY
  # ---------------------------------------------------------------------------

  "qwen3-coder*":
    # Training capabilities (what the model was trained to do)
    training:
      tool_calling: true
      thinking_mode: true
      thinking_disable_prefix: "/no_think"  # Qwen3 prefix to skip thinking
      code_generation: true

    # Provider-specific support
    providers:
      ollama:
        native_tool_calls: false  # NO {{ if .Tools }} in Ollama template
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true   # llama.cpp enables it
      vllm:
        native_tool_calls: true   # Guided decoding enables it

    # Model settings
    settings:
      recommended_tool_budget: 10
      argument_format: xml

  "qwen3-coder-tools*":
    # Custom -tools variant with Modelfile tool template
    training:
      tool_calling: true
      thinking_mode: true
      thinking_disable_prefix: "/no_think"  # Qwen3 prefix to skip thinking
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true   # Custom Modelfile enables it
        parallel_tool_calls: true
        # Set to true for stricter prompts to prevent continuation without action
        requires_strict_prompting: true
        # How many empty responses before forcing continuation prompt
        continuation_patience: 3
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: xml
      # Stricter thresholds for this model
      stage_transition_confidence: 0.75

  "qwen3:*":
    # Base Qwen3 (non-coder) - HAS Ollama tool support
    training:
      tool_calling: true
      thinking_mode: true
      thinking_disable_prefix: "/no_think"  # Qwen3 prefix to skip thinking

    providers:
      ollama:
        native_tool_calls: true   # Has {{ if .Tools }} in template
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: xml

  "qwen2.5*":
    # Generic qwen2.5 - most have Ollama tool support
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true   # Has {{ if .Tools }} in template
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true
      cerebras:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: xml

  # qwen2.5-coder larger sizes - NO tool template in Ollama
  # NOTE: qwen2.5-coder has INCONSISTENT tool support across sizes:
  #   - qwen2.5-coder:1.5b HAS {{ if .Tools }} (uses generic qwen2.5* config)
  #   - qwen2.5-coder:7b HAS {{ if .Tools }} (uses generic qwen2.5* config)
  #   - qwen2.5-coder:14b does NOT have tool template
  #   - qwen2.5-coder:32b does NOT have tool template
  # These specific patterns override the generic qwen2.5* for larger sizes.
  "qwen2.5-coder:14b*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false  # NO {{ if .Tools }} in template
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true   # llama.cpp enables it
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 12
      argument_format: xml

  "qwen2.5-coder:32b*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false  # NO {{ if .Tools }} in template
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 12
      argument_format: xml

  "qwen2.5-coder-tools*":
    # Custom -tools variant with tool template added
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true   # Custom Modelfile enables it
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: xml

  # ---------------------------------------------------------------------------
  # LLAMA FAMILY
  # ---------------------------------------------------------------------------

  "llama3.1*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true   # Has {{ if .Tools }} in template
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true
      cerebras:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: json

  "llama3.2*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: json

  "llama3.3*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true
      cerebras:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: json

  "codellama*":
    training:
      tool_calling: false  # Limited tool training
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true   # LMStudio can enable it
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 8

  # ---------------------------------------------------------------------------
  # MISTRAL FAMILY
  # ---------------------------------------------------------------------------

  "mistral*":
    training:
      tool_calling: true   # Trained with tool support
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false  # NO {{ if .Tools }} in template
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true   # llama.cpp enables it
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "mistral-tools*":
    # Custom -tools variant
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true   # Custom Modelfile
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  "mixtral*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "mixtral-tools*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  "ministral*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "devstral*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "devstral-tools*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  # ---------------------------------------------------------------------------
  # DEEPSEEK FAMILY
  # ---------------------------------------------------------------------------

  "deepseek-coder*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "deepseek-coder-tools*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  "deepseek-coder-v2*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "deepseek-coder-v2-tools*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  "deepseek-r1*":
    training:
      tool_calling: true
      reasoning: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "deepseek-r1-tools*":
    training:
      tool_calling: true
      reasoning: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  # ---------------------------------------------------------------------------
  # GOOGLE FAMILY
  # ---------------------------------------------------------------------------

  "gemma3*":
    training:
      tool_calling: true   # Google trained with tool support
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false  # NO {{ if .Tools }} in template
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "gemma3-tools*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 12

  "gemma:*":
    # Legacy Gemma
    training:
      tool_calling: false

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 8

  # ---------------------------------------------------------------------------
  # MICROSOFT PHI FAMILY
  # ---------------------------------------------------------------------------

  "phi*":
    training:
      tool_calling: true   # Some tool capability
      reasoning: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
        parallel_tool_calls: false  # Smaller models may struggle
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 8

  "phi-tools*":
    training:
      tool_calling: true
      reasoning: true

    providers:
      ollama:
        native_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  # ---------------------------------------------------------------------------
  # COHERE FAMILY
  # ---------------------------------------------------------------------------

  "command-r*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: true   # Has tool support in template
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  # ---------------------------------------------------------------------------
  # FUNCTION CALLING SPECIALISTS
  # ---------------------------------------------------------------------------

  "hermes*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 18

  "functionary*":
    training:
      tool_calling: true   # Designed for function calling

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 20

  "firefunction*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  "gpt-oss*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true   # Has {{ if .Tools }} in template
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  # ---------------------------------------------------------------------------
  # OTHER MODELS
  # ---------------------------------------------------------------------------

  "yi*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "internlm*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  "jamba*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  # ---------------------------------------------------------------------------
  # MOONSHOT KIMI K2 FAMILY
  # ---------------------------------------------------------------------------
  # Kimi K2 is a trillion-parameter MoE model with 32B active parameters
  # and 256k context window. Supports native tool calling and reasoning traces.

  "kimi-k2-thinking*":
    training:
      tool_calling: true
      reasoning: true
      thinking_mode: true
      code_generation: true

    providers:
      moonshot:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 50
      recommended_tool_budget: 30
      argument_format: json

  "kimi-k2-instruct*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      moonshot:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 40
      recommended_tool_budget: 20
      argument_format: json

  # ---------------------------------------------------------------------------
  # DEEPSEEK API MODELS
  # ---------------------------------------------------------------------------
  # DeepSeek-V3.2 models with 128K context window
  # Note: deepseek-reasoner does NOT support function calling

  "deepseek-chat*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      deepseek:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 30
      recommended_tool_budget: 20
      argument_format: json

  "deepseek-reasoner*":
    training:
      tool_calling: false  # Reasoner does NOT support function calling
      reasoning: true
      thinking_mode: true
      code_generation: true

    providers:
      deepseek:
        native_tool_calls: false  # No function calling in reasoner
        streaming_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 0  # No tools
      recommended_tool_budget: 5  # Minimal, for exploration only
      argument_format: json

  # ---------------------------------------------------------------------------
  # GROQ CLOUD MODELS
  # ---------------------------------------------------------------------------
  # Ultra-fast inference via Groq LPU hardware
  # Free developer tier available at https://console.groq.com

  "llama-3.3-70b-versatile*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      groqcloud:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 50
      recommended_tool_budget: 25
      argument_format: json

  "llama-3.1-8b-instant*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      groqcloud:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 30
      recommended_tool_budget: 15
      argument_format: json

  "openai/gpt-oss-*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      groqcloud:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: false  # GPT-OSS doesn't support parallel calls
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 30
      recommended_tool_budget: 15
      argument_format: json

  "qwen/qwen3-32b*":
    training:
      tool_calling: true
      thinking_mode: true
      thinking_disable_prefix: "/no_think"
      code_generation: true

    providers:
      groqcloud:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 40
      recommended_tool_budget: 20
      argument_format: json

  "moonshotai/kimi-k2-instruct*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      groqcloud:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 50
      recommended_tool_budget: 30
      argument_format: json

  "meta-llama/llama-4-scout*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      groqcloud:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 40
      recommended_tool_budget: 20
      argument_format: json

  # ---------------------------------------------------------------------------
  # MISTRAL API MODELS
  # ---------------------------------------------------------------------------
  # Mistral AI direct API models (not Ollama)

  "mistral-large*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      mistral:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_max_tools: 40
      recommended_tool_budget: 20
      argument_format: json

  "mistral-small*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      mistral:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_max_tools: 30
      recommended_tool_budget: 15
      argument_format: json

  "codestral*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      mistral:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_max_tools: 35
      recommended_tool_budget: 18
      argument_format: json

  # ---------------------------------------------------------------------------
  # TOGETHER AI MODELS
  # ---------------------------------------------------------------------------

  "meta-llama/Meta-Llama*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      together:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_max_tools: 35
      recommended_tool_budget: 18
      argument_format: json

  "mistralai/*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      together:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_max_tools: 30
      recommended_tool_budget: 15
      argument_format: json

  # ---------------------------------------------------------------------------
  # FIREWORKS AI MODELS
  # ---------------------------------------------------------------------------

  "accounts/fireworks/models/llama*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      fireworks:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_max_tools: 35
      recommended_tool_budget: 18
      argument_format: json

  "accounts/fireworks/models/qwen*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      fireworks:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_max_tools: 35
      recommended_tool_budget: 18
      argument_format: json

  # ---------------------------------------------------------------------------
  # AZURE OPENAI MODELS
  # ---------------------------------------------------------------------------
  # Azure uses deployment names, which can be anything
  # Provider defaults apply for all Azure models

  "gpt-4*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      azure:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
      openai:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_max_tools: 50
      recommended_tool_budget: 25
      argument_format: json

  "gpt-3.5*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      azure:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
      openai:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_max_tools: 30
      recommended_tool_budget: 15
      argument_format: json

  # ---------------------------------------------------------------------------
  # ANTHROPIC CLAUDE MODELS
  # ---------------------------------------------------------------------------

  "claude-*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      anthropic:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
      bedrock:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_max_tools: 50
      recommended_tool_budget: 25
      argument_format: json

  # ---------------------------------------------------------------------------
  # GOOGLE GEMINI MODELS
  # ---------------------------------------------------------------------------

  "gemini-*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      google:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
      vertex:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_max_tools: 40
      recommended_tool_budget: 20
      argument_format: json

  # ---------------------------------------------------------------------------
  # XAI GROK MODELS
  # ---------------------------------------------------------------------------

  "grok-*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      xai:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true

    settings:
      recommended_max_tools: 40
      recommended_tool_budget: 20
      argument_format: json

# =============================================================================
# TOOL SERIALIZATION CONFIGURATION
# =============================================================================
# Tool-specific serialization preferences and output type hints.
# This enables smarter format selection based on what each tool produces.
#
# Output types:
#   - tabular: List of uniform objects (databases, search results)
#   - text: Plain text content (file content, bash output)
#   - structured: Nested JSON structures
#   - mixed: Variable output depending on operation

tool_serialization:
  # Default preferences for all tools
  defaults:
    output_type: mixed
    preferred_format: null  # auto-select
    min_rows_for_tabular: 3  # Minimum rows to use TOON/CSV
    serialization_enabled: true

  # Tool categories with shared configuration
  categories:
    # Database and data tools - strongly prefer tabular formats
    data_tools:
      tools:
        - database_tool
        - sql_query
        - csv_reader
      output_type: tabular
      preferred_formats:
        - csv
        - toon
        - json_minified
      min_savings_threshold: 0.15

    # Search tools - often return structured results
    search_tools:
      tools:
        - code_search
        - semantic_code_search
        - web_search
      output_type: tabular
      preferred_formats:
        - toon
        - json_minified

    # File tools - text content, no serialization
    file_tools:
      tools:
        - read_file
        - list_directory
      output_type: text
      serialization_enabled: false  # Keep as text

    # Bash/command tools - text output
    command_tools:
      tools:
        - execute_bash
        - docker
      output_type: text
      serialization_enabled: false

    # Git tools - mixed output (some tabular, some text)
    git_tools:
      tools:
        - git
      output_type: mixed
      preferred_formats:
        - toon
        - json_minified

    # Refactoring/analysis tools - structured JSON output
    analysis_tools:
      tools:
        - refactor
        - code_review
        - metrics
        - dependency_analysis
      output_type: structured
      preferred_formats:
        - json_minified
        - json

  # Individual tool overrides (highest priority)
  tools:
    # Database tool always returns tabular data
    database_tool:
      output_type: tabular
      preferred_format: csv
      min_rows_for_tabular: 2

    # Code search returns file:line matches
    code_search:
      output_type: tabular
      preferred_format: toon

    # Semantic search returns ranked results
    semantic_code_search:
      output_type: tabular
      preferred_format: toon

    # List directory is already compact
    list_directory:
      serialization_enabled: false

    # Read file should never be serialized (code needs verbatim)
    read_file:
      serialization_enabled: false

    # Bash output is text
    execute_bash:
      serialization_enabled: false

    # Web search returns structured results
    web_search:
      output_type: tabular
      preferred_format: toon
      include_format_hint: true

    # Git log/status can be tabular
    git:
      output_type: mixed
      # Only serialize for log/status/branch operations
      serialize_operations:
        - log
        - status
        - branch
      skip_operations:
        - diff
        - show
