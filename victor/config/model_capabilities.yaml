# Model Capabilities Configuration v0.1.0
#
# MODEL-CENTRIC SCHEMA: Each model is the authoritative source of truth.
# Helper tools can update specific sections for individual models.
#
# Schema:
#   models.<model_pattern>:
#     training:        # What the model was trained to do (provider-independent)
#     providers:       # How each provider enables these capabilities
#     settings:        # Tuning parameters
#
# Resolution order:
#   1. defaults
#   2. provider_defaults.<provider>
#   3. models.<pattern>.training (inherent capabilities)
#   4. models.<pattern>.providers.<provider> (provider-specific overrides)
#   5. models.<pattern>.settings (tuning)

schema_version: "0.1.0"

# =============================================================================
# GLOBAL DEFAULTS
# =============================================================================
defaults:
  # Default capability assumptions for unknown models
  training:
    tool_calling: false
    thinking_mode: false
    code_generation: false
    vision: false
    reasoning: false

  # Default provider support
  provider_support:
    native_tool_calls: false
    streaming_tool_calls: false
    parallel_tool_calls: false
    tool_choice_param: false

  # Default fallback parsing (when native support unavailable)
  fallback:
    json_parsing: true
    xml_parsing: true

  # Default settings
  settings:
    recommended_max_tools: 15
    recommended_tool_budget: 10
    requires_strict_prompting: true
    argument_format: json
    # Exploration multiplier: models that need more "thinking" turns get higher values
    # This multiplies the max_exploration_iterations limit
    exploration_multiplier: 1.0
    # Continuation patience: how many empty turns to allow before forcing completion
    continuation_patience: 3

# =============================================================================
# PROVIDER DEFAULTS
# =============================================================================
# Infrastructure-level capabilities for each provider.
# These define what the provider CAN support, not what individual models do.

provider_defaults:
  # Cloud providers - full native support via their APIs
  anthropic:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 50
    recommended_tool_budget: 20

  openai:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 50
    recommended_tool_budget: 20

  google:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 15

  xai:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 15

  # Ollama - requires {{ if .Tools }} in Modelfile template
  ollama:
    native_tool_calls: false  # Must be enabled per-model
    streaming_tool_calls: false
    parallel_tool_calls: false
    tool_choice_param: false
    requires_strict_prompting: true
    recommended_max_tools: 20
    recommended_tool_budget: 12

  # LMStudio - llama.cpp enables tools for ALL models (0.3.6+)
  lmstudio:
    native_tool_calls: true
    streaming_tool_calls: false
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 25
    recommended_tool_budget: 15

  # vLLM - guided decoding via Outlines for all models
  vllm:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 15

  # Moonshot AI - OpenAI-compatible API for Kimi K2 models
  moonshot:
    native_tool_calls: true
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 40
    recommended_tool_budget: 20

  # DeepSeek API - OpenAI-compatible with reasoning support
  deepseek:
    native_tool_calls: true  # Only deepseek-chat, not deepseek-reasoner
    streaming_tool_calls: true
    parallel_tool_calls: true
    tool_choice_param: true
    requires_strict_prompting: false
    recommended_max_tools: 30
    recommended_tool_budget: 20
    # DeepSeek models need more "thinking" turns, give them higher exploration limit
    exploration_multiplier: 1.5
    continuation_patience: 5

# =============================================================================
# MODEL CONFIGURATIONS
# =============================================================================
# Each model pattern is authoritative. Helper tools update specific sections.

models:
  # ---------------------------------------------------------------------------
  # QWEN FAMILY
  # ---------------------------------------------------------------------------

  "qwen3-coder*":
    # Training capabilities (what the model was trained to do)
    training:
      tool_calling: true
      thinking_mode: true
      thinking_disable_prefix: "/no_think"  # Qwen3 prefix to skip thinking
      code_generation: true

    # Provider-specific support
    providers:
      ollama:
        native_tool_calls: false  # NO {{ if .Tools }} in Ollama template
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true   # llama.cpp enables it
      vllm:
        native_tool_calls: true   # Guided decoding enables it

    # Model settings
    settings:
      recommended_tool_budget: 10
      argument_format: xml

  "qwen3-coder-tools*":
    # Custom -tools variant with Modelfile tool template
    training:
      tool_calling: true
      thinking_mode: true
      thinking_disable_prefix: "/no_think"  # Qwen3 prefix to skip thinking
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true   # Custom Modelfile enables it
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: xml

  "qwen3:*":
    # Base Qwen3 (non-coder) - HAS Ollama tool support
    training:
      tool_calling: true
      thinking_mode: true
      thinking_disable_prefix: "/no_think"  # Qwen3 prefix to skip thinking

    providers:
      ollama:
        native_tool_calls: true   # Has {{ if .Tools }} in template
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: xml

  "qwen2.5*":
    # Generic qwen2.5 - most have Ollama tool support
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true   # Has {{ if .Tools }} in template
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: xml

  # qwen2.5-coder larger sizes - NO tool template in Ollama
  # NOTE: qwen2.5-coder has INCONSISTENT tool support across sizes:
  #   - qwen2.5-coder:1.5b HAS {{ if .Tools }} (uses generic qwen2.5* config)
  #   - qwen2.5-coder:7b HAS {{ if .Tools }} (uses generic qwen2.5* config)
  #   - qwen2.5-coder:14b does NOT have tool template
  #   - qwen2.5-coder:32b does NOT have tool template
  # These specific patterns override the generic qwen2.5* for larger sizes.
  "qwen2.5-coder:14b*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false  # NO {{ if .Tools }} in template
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true   # llama.cpp enables it
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 12
      argument_format: xml

  "qwen2.5-coder:32b*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false  # NO {{ if .Tools }} in template
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 12
      argument_format: xml

  "qwen2.5-coder-tools*":
    # Custom -tools variant with tool template added
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true   # Custom Modelfile enables it
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: xml

  # ---------------------------------------------------------------------------
  # LLAMA FAMILY
  # ---------------------------------------------------------------------------

  "llama3.1*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true   # Has {{ if .Tools }} in template
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: json

  "llama3.2*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: json

  "llama3.3*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15
      argument_format: json

  "codellama*":
    training:
      tool_calling: false  # Limited tool training
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true   # LMStudio can enable it
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 8

  # ---------------------------------------------------------------------------
  # MISTRAL FAMILY
  # ---------------------------------------------------------------------------

  "mistral*":
    training:
      tool_calling: true   # Trained with tool support
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false  # NO {{ if .Tools }} in template
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true   # llama.cpp enables it
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "mistral-tools*":
    # Custom -tools variant
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true   # Custom Modelfile
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  "mixtral*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "mixtral-tools*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  "ministral*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "devstral*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "devstral-tools*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  # ---------------------------------------------------------------------------
  # DEEPSEEK FAMILY
  # ---------------------------------------------------------------------------

  "deepseek-coder*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "deepseek-coder-tools*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  "deepseek-coder-v2*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "deepseek-coder-v2-tools*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  "deepseek-r1*":
    training:
      tool_calling: true
      reasoning: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "deepseek-r1-tools*":
    training:
      tool_calling: true
      reasoning: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  # ---------------------------------------------------------------------------
  # GOOGLE FAMILY
  # ---------------------------------------------------------------------------

  "gemma3*":
    training:
      tool_calling: true   # Google trained with tool support
      code_generation: true

    providers:
      ollama:
        native_tool_calls: false  # NO {{ if .Tools }} in template
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "gemma3-tools*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 12

  "gemma:*":
    # Legacy Gemma
    training:
      tool_calling: false

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 8

  # ---------------------------------------------------------------------------
  # MICROSOFT PHI FAMILY
  # ---------------------------------------------------------------------------

  "phi*":
    training:
      tool_calling: true   # Some tool capability
      reasoning: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
        parallel_tool_calls: false  # Smaller models may struggle
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 8

  "phi-tools*":
    training:
      tool_calling: true
      reasoning: true

    providers:
      ollama:
        native_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  # ---------------------------------------------------------------------------
  # COHERE FAMILY
  # ---------------------------------------------------------------------------

  "command-r*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: true   # Has tool support in template
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  # ---------------------------------------------------------------------------
  # FUNCTION CALLING SPECIALISTS
  # ---------------------------------------------------------------------------

  "hermes*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 18

  "functionary*":
    training:
      tool_calling: true   # Designed for function calling

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 20

  "firefunction*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  "gpt-oss*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      ollama:
        native_tool_calls: true   # Has {{ if .Tools }} in template
        parallel_tool_calls: true
        requires_strict_prompting: false
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  # ---------------------------------------------------------------------------
  # OTHER MODELS
  # ---------------------------------------------------------------------------

  "yi*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 10

  "internlm*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  "jamba*":
    training:
      tool_calling: true

    providers:
      ollama:
        native_tool_calls: false
        requires_strict_prompting: true
      lmstudio:
        native_tool_calls: true
      vllm:
        native_tool_calls: true

    settings:
      recommended_tool_budget: 15

  # ---------------------------------------------------------------------------
  # MOONSHOT KIMI K2 FAMILY
  # ---------------------------------------------------------------------------
  # Kimi K2 is a trillion-parameter MoE model with 32B active parameters
  # and 256k context window. Supports native tool calling and reasoning traces.

  "kimi-k2-thinking*":
    training:
      tool_calling: true
      reasoning: true
      thinking_mode: true
      code_generation: true

    providers:
      moonshot:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 50
      recommended_tool_budget: 30
      argument_format: json

  "kimi-k2-instruct*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      moonshot:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 40
      recommended_tool_budget: 20
      argument_format: json

  # ---------------------------------------------------------------------------
  # DEEPSEEK API MODELS
  # ---------------------------------------------------------------------------
  # DeepSeek-V3.2 models with 128K context window
  # Note: deepseek-reasoner does NOT support function calling

  "deepseek-chat*":
    training:
      tool_calling: true
      code_generation: true

    providers:
      deepseek:
        native_tool_calls: true
        streaming_tool_calls: true
        parallel_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 30
      recommended_tool_budget: 20
      argument_format: json

  "deepseek-reasoner*":
    training:
      tool_calling: false  # Reasoner does NOT support function calling
      reasoning: true
      thinking_mode: true
      code_generation: true

    providers:
      deepseek:
        native_tool_calls: false  # No function calling in reasoner
        streaming_tool_calls: true
        requires_strict_prompting: false

    settings:
      recommended_max_tools: 0  # No tools
      recommended_tool_budget: 5  # Minimal, for exploration only
      argument_format: json
