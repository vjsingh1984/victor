# Provider Context Limits Configuration
#
# This file defines context window limits and other constraints for LLM providers.
# Edit this file to add new providers or adjust limits.
#
# Note: Values can be overridden per-profile in profiles.yaml
#
# Format:
#   provider_name:
#     context_window: <max tokens>
#     response_reserve: <tokens to reserve for response>
#     supports_extended_context: <bool>

providers:
  # Cloud Providers
  anthropic:
    context_window: 200000
    response_reserve: 8192
    supports_extended_context: true
    rate_limit_rpm: 60
    rate_limit_tpm: 100000

  openai:
    context_window: 128000
    response_reserve: 4096
    supports_extended_context: true
    rate_limit_rpm: 60
    rate_limit_tpm: 90000

  google:
    context_window: 1000000  # Gemini 1.5 Pro
    response_reserve: 8192
    supports_extended_context: true
    rate_limit_rpm: 60
    rate_limit_tpm: 100000

  xai:
    context_window: 131072
    response_reserve: 4096
    supports_extended_context: true
    rate_limit_rpm: 60
    rate_limit_tpm: 100000

  deepseek:
    context_window: 64000
    response_reserve: 4096
    supports_extended_context: false
    rate_limit_rpm: 60
    rate_limit_tpm: 100000

  moonshot:
    context_window: 128000
    response_reserve: 4096
    supports_extended_context: true
    rate_limit_rpm: 60
    rate_limit_tpm: 100000

  groqcloud:
    context_window: 131072
    response_reserve: 8192
    supports_extended_context: true
    rate_limit_rpm: 30  # Groq has stricter limits
    rate_limit_tpm: 20000

  # Local Providers (no rate limits)
  ollama:
    context_window: 65536
    response_reserve: 4096
    supports_extended_context: true
    rate_limit_rpm: 0  # Unlimited
    rate_limit_tpm: 0

  lmstudio:
    context_window: 65536
    response_reserve: 4096
    supports_extended_context: true
    rate_limit_rpm: 0
    rate_limit_tpm: 0

  vllm:
    context_window: 65536
    response_reserve: 4096
    supports_extended_context: true
    rate_limit_rpm: 0
    rate_limit_tpm: 0

# Model-specific overrides
# These take precedence over provider defaults
models:
  "gpt-4-turbo*":
    context_window: 128000
  "gpt-4o*":
    context_window: 128000
  "gpt-4*":
    context_window: 8192
  "gpt-3.5-turbo*":
    context_window: 16385
  "claude-3-opus*":
    context_window: 200000
  "claude-3-sonnet*":
    context_window: 200000
  "claude-3-haiku*":
    context_window: 200000
  "gemini-1.5-pro*":
    context_window: 1000000
  "gemini-1.5-flash*":
    context_window: 1000000
  "llama*70b*":
    context_window: 131072
  "qwen*32b*":
    context_window: 131072
  "mixtral*":
    context_window: 32768
