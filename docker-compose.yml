# Copyright 2025 Vijaykumar Singh <singhvjd@gmail.com>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

services:
  # Ollama - Local LLM server
  ollama:
    image: ollama/ollama:latest
    container_name: victor-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    networks:
      - victor-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    # Uncomment for NVIDIA GPU support (Linux only)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    profiles:
      - full
      - ollama
      - demo
      - notebook

  # vLLM - High-performance inference server
  vllm:
    image: vllm/vllm-openai:latest
    container_name: victor-vllm
    ports:
      - "8000:8000"
    environment:
      - VLLM_MODEL=Qwen/Qwen2.5-Coder-1.5B-Instruct
    command: >
      --model Qwen/Qwen2.5-Coder-1.5B-Instruct
      --dtype float16
      --max-model-len 2048
      --port 8000
      --enable-auto-tool-choice
      --tool-call-parser hermes
    volumes:
      - vllm_cache:/root/.cache/huggingface
    networks:
      - victor-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    # Uncomment for NVIDIA GPU support (Linux only)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    profiles:
      - full
      - vllm

  # Victor - Main application
  victor:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: victor-app
    volumes:
      - ./examples:/app/examples:ro
      - ./docs:/app/docs:ro
      - victor_home:/home/victor/.victor
      - ./demo_workspace:/workspace
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - VLLM_API_BASE=http://vllm:8000/v1
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - XAI_API_KEY=${XAI_API_KEY:-}
    networks:
      - victor-network
    depends_on:
      - ollama
    working_dir: /workspace
    stdin_open: true
    tty: true
    command: bash
    profiles:
      - full
      - demo

  # Demo runner - Automated demonstrations
  victor-demo:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: victor-demo
    volumes:
      - ./examples:/app/examples:ro
      - ./docker/demos:/app/demos:ro
      - demo_output:/output
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - VLLM_API_BASE=http://vllm:8000/v1
      - DEMO_OUTPUT_DIR=/output
    networks:
      - victor-network
    depends_on:
      - ollama
    command: python /app/demos/run_all_demos.py
    profiles:
      - demo

  # Jupyter notebook for interactive demos
  jupyter:
    image: jupyter/base-notebook:latest
    container_name: victor-jupyter
    ports:
      - "8888:8888"
    volumes:
      - ./examples:/home/jovyan/examples:ro
      - ./notebooks:/home/jovyan/notebooks
      - jupyter_data:/home/jovyan/.local
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - VLLM_API_BASE=http://vllm:8000/v1
      - JUPYTER_ENABLE_LAB=yes
    networks:
      - victor-network
    depends_on:
      - ollama
    profiles:
      - full
      - notebook

networks:
  victor-network:
    driver: bridge

volumes:
  ollama_data:
    name: victor_ollama_data
  vllm_cache:
    name: victor_vllm_cache
  victor_home:
    name: victor_home
  demo_output:
    name: victor_demo_output
  jupyter_data:
    name: victor_jupyter_data
